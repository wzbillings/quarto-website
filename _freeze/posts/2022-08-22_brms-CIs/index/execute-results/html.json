{
  "hash": "4fb529b02958a32d7a58c35e7458fb1d",
  "result": {
    "markdown": "---\ntitle: \"Doing what brms does\"\nauthor: \"Zane Billings\"\ndate: \"2022-08-22\"\ndescription: |\n  If you're anything like me, when you got predictions from a brms model with a\n  CI before, you probably thought. How can I do this manually, making it much\n  more error prone and tedious? Well, here's the answer! Maybe.\nlicense: \"CC BY SA\"\ndraft: true\n---\n\n\n\n\n![Two negative Abbot BinaxNOW Covid-19 antigen test cards on a dark wooden table, next to a kitchen timer shaped like a chicken.](thumbnail.jpg)\n\n# The backstory for this blog post\n\nLike most of my personality traits, my propensity to get into the weeds with every project I engage with is both a strength and a weakness. It really depends on the circumstances, I guess. Recently I've been working with my colleague [(Dr.) Yang Ge](https://twitter.com/yangepi) on one of his virus modeling projects. The analyses for this project all used Bayesian linear mixed-effects models, and while I know a decent amount about linear models, I don't know very much about Bayesian...anything.\n\nOf course knowing that I ~~work best~~ get slightly more work done under pressure, my mentor [Andreas](https://twitter.com/andreashandel) thought it would be a good idea for me to reproduce Yang's work using the `rethinking` package, which forces the user to write parametric bayesian models out in much more detail than `brms` does. After months of work and a few more months of quiet contemplation, I have decided to forgive Andreas for this.\n\nThe low-down, I guess, is that `brms` is much more convenient for people who know what they are doing, but it is much easier to make mistakes or make accidental unplanned assumptions. As I am not one such person who knows what they are doing, I looked at Yang's `brms` code and said \"uhhhh...okay.\" So we (mostly Yang and Andreas, but sometimes me since I took like four math stats classes or whatever) spent some time writing out the probability models, and then I started implementing those in `rethinking`. Fortunately, the `rethinking` syntax is almost exactly like the normal syntax for writing out probabilistic models.\n\nLong story short, after a bit of struggling (as one does), I got the models to fit and I had gigabytes and gigabytes of posterior samples saved to my hard drive. And then I had to figure out how to get the posterior mean and credible interval estimates.\n\n# `brms` and credible intervals (backstory part 2)\n\nThat should be easy, right? Or so I thought, in my hubris. For a fixed-effects only model, we would just take the sample mean and use the empirical quantiles to construct a CI---as an avid bootstrap user, I've done this plenty of times. However, the random effects make this a bit more complicated. If you build your model with `brms`, the package writers have written the excellent `predict.brmsfit()` and `posterior_pred()` and similar functions that calculate the credible intervals and handle random effects for you. Of course, I was not using `brms`, so for me this was not an option. I had a matrix full of numbers to my name and nothing else.\n\nWhile I love the `rethinking` package for its quick interface with Stan, I note that the `rethinking::PI()` function for getting intervals using the quantiles of the samples does not abstract random effects away for the user. Personally, I think this is a good thing---I am never trustworthy of software that I do not understand.\n\nOne of the biggest annoyances I had during this project was that **there are no other references explaining how you calculate these CIs by hand!** I checked Statistical Rethinking, Data Analysis Using Regression and Multilevel/Hierarchical Models, AND BDA3. Either I don't know what I'm looking for, or the explanation for calculating these CI's just isn't in any of those. If anyone can recommend a book that covers this material better, ***PLEASE*** get in touch with me!\n\nSo, I started with [one of the world's best-ever blog posts](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/) by [Andrew Heiss](https://twitter.com/andrewheiss). I have had the pleasure of meeting Andrew once, but that was before I read this blog post, so I didn't get to tell him in person how this blog post saved my career, cleared my skin, fertilized my crops, etc. (Maybe not, but it sure was helpful.)\n\nSo, now that I've spent a lot of time rambling (another one of my personality traits that is sometimes a strength but often not), I'll get to the actual tutorial.\n\n# Who this guide is for\n\nI thought I should start by borrowing from Andrew, who was borrowing from [Solomon Kurtz](https://solomonkurz.netlify.app/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/). I'll make these assumptions.\n\n* You're familiar with [R]() and the [tidyverse](). I plan to do most of this in base R, but if it's more convenient I'll switch to tidy-style.\n\n* You're at least passingly familiar with Bayesian parametric probability models. If you've had a probability theory course (or know the equivalent material), I hope this will be understandable.\n\n* You have passing familiarity with random effects. Nothing too deep here, but I'll be building a model with a random intercept and adaptive priors, as described by McElreath in Chapter 12 of Statistical Rethinking.\n\n# The actual tutorial that everyone came here for, I assume\n\nSo, here's a brief outline of what I'll cover in this tutorial.\n\n1. Fitting an easy model using the `rethinking` package. This model will have a random intercept. If you're looking for models with more than one random effect, I'm not sure I can help you, as I haven't painstakingly worked through that yet! So if that's you, use this guide at your own risk.\n2. I'll show how to get the posterior samples from the model. Then I'll walk through manual calculations of the four types of credible intervals discussed in Andrew Heiss' blog post above.\n3. Hopefully I'll refit the model with `brms`, this might come a bit later after this is posted. Depends on how fast I can absorb Yang's `brms` knowledge from his code. That way we can check and see that my answers are more-or-less correct.\n\nEnough messing about, let's get started!\n\n## Fitting a model with `rethinking`\n\n### Example data and model\n\nFor this type of simple one random-intercept model, I'll use the `iris` dataset that everyone is familiar with. If this doesn't work out how I want, I'll probably come back here and either find a different dataset to use, or generate some fake data that matches the hierarchical model I want to fit (of course with some noise üòÅ).\n\nBut what I'm interested in here is one continuous outcome with one continuous predictor, with a categorical grouping factor. And `iris` works for that!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\nFor convenience, I'll call `Petal.Length` $y$ (our outcome of interest) and `Sepal.Width` $x$ (our predictor, I choose this because I thought the scatterplot looked interesting). We'll use `Species` as an indexing variable, $i$ here. That means we'll have different estimates for each species in the dataset.\n\nWe'll fit the following model.\n\n\\begin{align*}\nY_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{\\text{Species}[i]} + \\beta x \\\\\n\\alpha_i &\\sim \\text{Normal}(\\bar{\\alpha}, \\gamma), \\quad \\text{ for } j = 1, \\ldots, 3 \\\\\n\\bar{\\alpha} &\\sim \\text{Normal}(0, 2) \\\\\n\\beta &\\sim \\text{Normal}(0, 2) \\\\\n\\gamma &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\n\nThis isn't a blog post about choosing principled or correct priors, so all I can really say about these is that I made them up after looking at Chapter 13 of Statistical Rethinking. They probably aren't very good and I should really do better, but that's not my main focus here, so I won't. I sort of just picked gamma likelihood and exponential/gamma priors because they are constrained to be positive, and since both our independent and dependent variables must be positive, I think it's probably fine to force all of these things to be positive. (In reality, only mu has to be positive and the $\\beta$ parameter has to be negative, but since we can plot the relationship and see that there's a positive correlation, I think this assumption is fine.)\n\nAnyways. Other people know lots more about priors than I do (like the Stan dev team and Michael Betancourt) so you should read their stuff about priors if that's what you're curious about. We should also probably do a prior predictive check, but again, I just want to talk about CIs, so let's move on.\n\n### Model fitting in R\n\nI'll fit the model using `rethinking`, since I think it's actually quite a nice interface to `rstan`, and I like that we basically enter the model just like it looks above. We'll do some minor data processing so I can be sure that I know which species is which in the output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rethinking)\n\nd <-\n\tiris |>\n\tdplyr::transmute(\n\t\tpetal_length = Petal.Length,\n\t\tsepal_width = Sepal.Width,\n\t\tspecies = as.numeric(Species)\n\t) |>\n\tas.list()\n\nfit <-\n\trethinking::ulam(\n\t\tflist = alist(\n\t\t\tpetal_length ~ gamma(mu, sigma),\n\t\t\tmu <- alpha[species] + beta * sepal_width,\n\t\t\talpha[species] ~ gamma(bar_alpha, gamma),\n\t\t\tbar_alpha ~ exponential(1),\n\t\t\tbeta ~ exponential(1),\n\t\t\tgamma ~ exponential(1),\n\t\t\tsigma ~ exponential(1)\n\t\t),\n\t\tdata = d,\n\t\tchains = 4,\n\t\tcores = 4,\n\t\titer = 5000,\n\t\twarmup = 2500,\n\t\tcmdstan = TRUE, # Not sure this is actually working\n\t\tcontrol = list(adapt_delta = 0.95, seed = 370)\n\t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 2 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 3 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 4 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 2 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 2 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 3 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 4 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 1 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 1 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 2 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 3 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 4 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 2 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 2 finished in 10.8 seconds.\nChain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 1 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 finished in 13.9 seconds.\nChain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 3 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 3 finished in 14.7 seconds.\nChain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 4 finished in 15.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 13.7 seconds.\nTotal execution time: 15.5 seconds.\n```\n:::\n:::\n\n\nNot sure if all the messages from the stan fit will show up here or not, hopefully not. But the model ran for me, I just got some warnings from how `rethinking` writes Stan syntax. Hopefully those warnings don't matter much `emoji::emoji(\"smile\")`.\n\nThe focus of this post isn't diagnostics either, but let's do a quick check.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::precis(fit, depth = 2) |>\n\tdata.frame() |>\n\tknitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n|          | mean|   sd| X5.5.| X94.5.|   n_eff| Rhat4|\n|:---------|----:|----:|-----:|------:|-------:|-----:|\n|alpha[1]  | 0.47| 0.25|  0.07|   0.87| 2169.24|     1|\n|alpha[2]  | 3.44| 0.21|  3.10|   3.78| 2259.86|     1|\n|alpha[3]  | 4.67| 0.22|  4.31|   5.03| 2260.87|     1|\n|bar_alpha | 2.08| 0.86|  0.86|   3.54| 4611.90|     1|\n|beta      | 0.29| 0.07|  0.17|   0.41| 2161.87|     1|\n|gamma     | 1.75| 0.93|  0.68|   3.50| 5381.65|     1|\n|sigma     | 0.04| 0.00|  0.03|   0.05| 6232.82|     1|\n:::\n:::\n\n\nThe effective sample sizes are pretty good, everything is higher than the actual sample size (2500) except for beta. But the ESS for beta is still pretty good. The $\\hat{R}$ values also look good. Every book on using Stan cautions on just looking at these metrics, but if you want something better...look in those books (BDA3, SR, etc.) since they explain it better than I can. So I'll assume we're good and finally get to what I actually wanted to talk about.\n\n## Estimating the posterior mean and its CI\n\nNow, we need to sample from the posterior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- rethinking::extract.samples(fit, n = 2500)\nstr(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ alpha    : num [1:2500, 1:3] 0.438 0.904 0.213 0.626 0.342 ...\n $ bar_alpha: num [1:2500(1d)] 1.91 3.03 2.68 2.41 1.03 ...\n $ beta     : num [1:2500(1d)] 0.304 0.16 0.372 0.237 0.331 ...\n $ gamma    : num [1:2500(1d)] 2.3 1.12 2.81 1.34 1.29 ...\n $ sigma    : num [1:2500(1d)] 0.0427 0.0433 0.0445 0.0371 0.0324 ...\n - attr(*, \"source\")= chr \"ulam posterior: 2500 samples from object\"\n```\n:::\n:::\n\n\nOk, so we have all of our samples, yay. You can see that we have three alpha vectors in a matrix (one for each species, in the order they showed up in the original dataset). Everything else is a vector of length 2500 (the number of samples we did). Here's some quick histograms showing the posterior distribution of each parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(post, {\n\tlayout(matrix(c(1, 1, 1, 2, 3, 4), ncol = 3, byrow = TRUE))\n\thist(bar_alpha, breaks = \"FD\", main = \"bar_alpha\")\n\thist(alpha[, 1], breaks = \"FD\", main = \"alpha (group 1)\")\n\thist(alpha[, 2], breaks = \"FD\", main = \"alpha (group 2)\")\n\thist(alpha[, 3], breaks = \"FD\", main = \"alpha (group 3)\")\n})\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(post, {\n\tlayout(matrix(c(1, 2, 3), ncol = 1, byrow = TRUE))\n\thist(beta,  breaks = \"FD\", main = \"beta\")\n\thist(sigma, breaks = \"FD\", main = \"sigma\")\n\thist(gamma, breaks = \"FD\", main = \"gamma\")\n})\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### \"Type One\" -- Grand means\n\nOk, in `brms` world, this would use the setting `re_formula = NA`. In short, **this is the variance about the mean, ignoring any variations in the random effects.** While discussing this with Andreas, I think a very subtle and important point is that *the estimate of the mean still includes information from the random effects* in this model formulation, but the credible interval will not take variation away from the mean of the random effects into account. I don't know that much about this formally, but my understanding is that this estimate is still conditional on the random effects. Because we used a \"partial pooling\" model, our estimate of `bar_alpha` includes random effects information. (If you read this and you think I'm wrong, please let me know!)\n\nBasically, the way this mean and CI is calculated is by **using only the mean of the random effects, ignoring RE-level deviations.** So for this example, that means we'll base our inference on `bar_alpha` rather than on the three `alpha` vectors. Now, we can of course do inference on just the `bar_alpha` parameter, but as Andrew Heiss pointed out in the blog posted I linked earlier, we can't just interpret it like we can in a frequentist OLS model.\n\nWhat we're trying to estimate here is how the expected value of the outcome (petal length) changes along with sepal width. In our model we defined\n$$\\hat{\\mu} = \\hat{\\alpha}_i + \\hat{\\beta} x,$$\nwhere $x$ is the predictor, sepal width. So what we need to do is use our posterior samples to calculate this mean. In lots of models, we would need to \"unlink\" in this step also (apply the inverse link function), but here I used an identity link, even though that's not too common with a gamma likelihood model, to make my life easier.\n\nSo we want to get predictions for a bunch of interpolated values of sepal width. We'll need to do a bit of vector/matrix manipulation to do this in R, so I'll try to explain that.\n\nFirst of all, we have our vectors of samples, $\\hat{\\beta}$ and $\\hat{\\bar{\\alpha}}$ (remember for this type of CI, we assume that $\\alpha_i = \\bar{\\alpha}$). These are both $2500 \\times 1$ column vectors. Now, our vector of interpolated $x$ values, say $\\tilde{x}$, will be a $j \\times 1$ column vector, where $j$ depends on interpolation. Clearly this is incompatible with our other vectors.\n\nHowever, we can use the [outer matrix product](https://en.wikipedia.org/wiki/Outer_product) to get this in the correct shape. Then, the formula for computing the linear predictor is\n$$\\hat{\\mu} =  \\hat{\\bar{\\alpha}} \\otimes \\mathbf{1} + \\hat{\\beta} \\otimes \\tilde{x},$$\nwhere $\\mathbf{1}$ is a column vector of $1$'s with dimensions $j \\times 1$. You can write this without the outer product by using matrix products and transposes, but I personally prefer this notation. If you're more comfortable without the outer product, this can also be written as\n\n$$\\hat{\\mu} = \\hat{\\bar{\\alpha}} \\mathbf{1}^\\prime + \\tilde{x}\\hat{\\beta}^\\prime, $$\nwhere multiplication is the standard matrix product.\n\nNow, both $\\hat{\\bar{\\alpha}} \\otimes \\mathbf{1}$ and $\\hat{\\beta} \\otimes \\tilde{x}$ will be $2500 \\times j$ matrices (and so are conformable for addition) and will look like this:\n\n$$\\begin{bmatrix} \\hat{\\bar{\\alpha}}_1 & \\hat{\\bar{\\alpha}}_1 & \\cdots & \\hat{\\bar{\\alpha}}_1 \\\\\n\\hat{\\bar{\\alpha}}_2 & \\hat{\\bar{\\alpha}}_2 & \\cdots & \\hat{\\bar{\\alpha}}_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\hat{\\bar{\\alpha}}_{2500} & \\hat{\\bar{\\alpha}}_{2500} & \\cdots & \\hat{\\bar{\\alpha}}_{2500}\n\\end{bmatrix} + \\begin{bmatrix} \n\\hat{\\beta}_1 \\tilde{x}_1 & \\hat{\\beta}_1 \\tilde{x}_2 & \\cdots & \\hat{\\beta}_1 \\tilde{x}_j \\\\\n\\hat{\\beta}_2 \\tilde{x}_1 & \\hat{\\beta}_2 \\tilde{x}_2 & \\cdots & \\hat{\\beta}_2 \\tilde{x}_j \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\hat{\\beta}_{2500} \\tilde{x}_1 & \\hat{\\beta}_{2500} \\tilde{x}_2 & \\cdots & \\hat{\\beta}_{2500} \\tilde{x}_j\n\\end{bmatrix} = \\begin{bmatrix}\n\\hat{\\bar{\\alpha}}_1 + \\hat{\\beta}_1 \\tilde{x}_1 & \\hat{\\bar{\\alpha}}_1 + \\hat{\\beta}_1 \\tilde{x}_2 & \\cdots & \\hat{\\bar{\\alpha}}_1 + \\hat{\\beta}_1 \\tilde{x}_j \\\\\n\\hat{\\bar{\\alpha}}_2 + \\hat{\\beta}_2 \\tilde{x}_1 & \\hat{\\bar{\\alpha}}_2 + \\hat{\\beta}_2 \\tilde{x}_2 & \\cdots & \\hat{\\bar{\\alpha}}_2 + \\hat{\\beta}_2 \\tilde{x}_j \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\hat{\\bar{\\alpha}}_{2500} + \\hat{\\beta}_{2500} \\tilde{x}_1 & \\hat{\\bar{\\alpha}}_{2500} + \\hat{\\beta}_{2500} \\tilde{x}_2 & \\cdots & \\hat{\\bar{\\alpha}}_{2500} + \\hat{\\beta}_{2500} \\tilde{x}_j \\\\\n\\end{bmatrix}.$$\n\nSo, you can see we get a linear predictor (since the link is the identity function) for each element of the matrix. We've constructed 2500 samples of the linear predictor evaluated at $j$ values of the predictor $x$. (Under the assumption that we don't care about group-level deviations from the mean. Remember, the whole reason this makes the type 1 CI is that we used $\\bar{\\alpha}$.)\n\nNow let's do this in `R` code. The first thing we have to do is construct the vector of sepal width values to use. In the code below, you can see that I made a vector that goes from the minimum observed value to the maximum observe value in steps of 0.01. Then, we can construct the $\\hat{\\mu}$ (not sure if this is the technically correct notation üòÅ) matrix using either of the matrix formulas I listed above. In R, `%*%` is the standard matrix product and `%o%` is the outer product.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_tilde <- seq(min(d$sepal_width), max(d$sepal_width), by = 0.01)\nones <- rep(1, times = length(x_tilde))\n\nwith(post, pred_vals <<- bar_alpha %o% ones + beta %o% x_tilde)\n# alternatively, pred_vals <- post$bar_alpha %o% ones + post$beta %o% x_tilde\nstr(pred_vals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:2500, 1:241] 2.51 3.34 3.43 2.89 1.69 ...\n```\n:::\n:::\n\n\nNow that we have these samples, we can summarize them in the typical way. For each column of the `pred_vals` matrix, you can estimate the sample mean (or other statistics) along with a credible interval. I'll use an equal-tailed (aka percentile) 89% credible interval, but you could also use the HDPI or whatever else. I also prefer to tidy up the data first before I do anything else.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a function to return the mean and percentile interval as a tibble\nmean_pi <- function(x, p = 0.89) {\n\test <- rethinking::PI(x, prob = p)\n\tout <- tibble::tibble(\n\t\tmean = mean(x),\n\t\tlower = est[1],\n\t\tupper = est[2]\n\t)\n\treturn(out)\n}\n\ntype_1_summary <-\n\tpred_vals |>\n\t# Convert from matrix to tibble\n\ttibble::as_tibble() |>\n\t# Summarize each column\n\tdplyr::summarize(\n\t\tdplyr::across(\n\t\t\t.cols = dplyr::everything(),\n\t\t\t.f = ~list(mean_pi(.x))\n\t\t)\n\t) |>\n\t# Set names so we can pivot longer easily\n\trlang::set_names(nm = x_tilde) |>\n\t# Pivot to get in the right form for ggplot\n\ttidyr::pivot_longer(\n\t\tcols = tidyselect::everything(),\n\t\tnames_to = \"sepal_width\",\n\t\tvalues_to = \"stats\",\n\t\tnames_transform = list(sepal_width = as.numeric)\n\t) |>\n\t# Unnest to get separate columns for mean and CI bounds\n\ttidyr::unnest(cols = stats)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nUsing compatibility `.name_repair`.\n```\n:::\n:::\n\n\nAlright, now that we've got the data cleaned up and the Type 1 CI computed, we can make a plot to show what this CI looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype_1_summary |>\n\tggplot(aes(x = sepal_width, y = mean, ymin = lower, ymax = upper)) +\n\t# This part plots the mean line and CI\n\tgeom_ribbon(alpha = 0.5, fill = \"gray\") +\n\tgeom_line(size = 1) +\n\t# This part plots the raw data points\n\tgeom_point(\n\t\tdata = iris,\n\t\taes(x = Sepal.Width, y = Petal.Length, color = Species,\n\t\t\tshape = Species),\n\t\tinherit.aes = FALSE, size = 2, stroke = 1\n\t) +\n\t# This plot customizes how the plot looks\n\tscale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n\tscale_shape_manual(values = c(21, 22, 24)) +\n\tcoord_cartesian(ylim = c(0, 7)) +\n\tlabs(x = \"\\nsepal width\", y = \"petal length\\n\") +\n\tzlib::theme_ms()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nHere we can see one of the problems that made me think `iris` may not be such a good data set for demonstrating these calculations -- the average fits the overall global mean well, but it doesn't describe any of the three groups particularly well. However, after seeing this, I think it might actually demonstrate the shortcomings of the global mean in cases like this. Anyways, we got the CI calculated manually, and now we can move on to the next CI type.\n\n### \"Type Two\" -- Group means\n\nThe CIs of type two (I hope that the order in Andrew Heiss' blog post becomes the canonical name for these four types of CIs, since I haven't seen any official names anywhere else) are group-specific, rather than calculating one overall estimate that combines groups. The formula for calculating these CIs is probably the easiest one to write down:\n\n$$\\hat{\\mu}_s =  \\hat{\\alpha}_s \\otimes \\mathbf{1} + \\hat{\\beta} \\otimes \\tilde{x},$$\nwhere $s \\in S = \\{\\text{setosa, versicolor, viginica}\\}$ indexes the species group. Since we're using all of the same matrix math as before, I won't rattle on as much this time. Instead we just need one extra ingredient for these CIs -- the incredibly versatile `purrr::map()`! I **KNOW** there's a better way to do this and the way I'm doing it here is weird and inefficient, I just didn't want to work on it longer and this runs pretty fast.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype_2_summary <-\n\t# Get the estimates for each group\n\tpurrr::map(\n\t\t1:3,\n\t\t~as.data.frame(post$alpha[, .x] %o% ones + post$beta %o% x_tilde)\n\t) |>\n\t# Set names so that the species value will be calculated correctly\n\trlang::set_names(nm = c(\"setosa\", \"versicolor\", \"virginica\")) |>\n\t# Combine the list of data frames into one data frame\n\tdplyr::bind_rows(.id = \"species\") |>\n\t# Do the summarization for each species\n\tdplyr::group_by(species) |>\n\t# Now calculate the mean and CI the same as before.\n\tdplyr::summarize(\n\t\tdplyr::across(\n\t\t\t.cols = dplyr::everything(),\n\t\t\t.f = ~list(mean_pi(.x))\n\t\t)\n\t) |>\n\trlang::set_names(nm = c(\"species\", x_tilde)) |>\n\ttidyr::pivot_longer(\n\t\tcols = -species,\n\t\tnames_to = \"sepal_width\",\n\t\tvalues_to = \"stats\",\n\t\tnames_transform = list(sepal_width = as.numeric)\n\t) |>\n\t# Unnest to get separate columns for mean and CI bounds\n\ttidyr::unnest(cols = stats)\n\nstr(type_2_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [723 √ó 5] (S3: tbl_df/tbl/data.frame)\n $ species    : chr [1:723] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n $ sepal_width: num [1:723] 2 2.01 2.02 2.03 2.04 2.05 2.06 2.07 2.08 2.09 ...\n $ mean       : num [1:723] 1.06 1.06 1.06 1.06 1.07 ...\n $ lower      : Named num [1:723] 0.885 0.889 0.893 0.897 0.901 ...\n  ..- attr(*, \"names\")= chr [1:723] \"5%\" \"5%\" \"5%\" \"5%\" ...\n $ upper      : Named num [1:723] 1.22 1.22 1.23 1.23 1.23 ...\n  ..- attr(*, \"names\")= chr [1:723] \"94%\" \"94%\" \"94%\" \"94%\" ...\n```\n:::\n:::\n\n\nNow let's plot these CIs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype_2_summary |>\n\tggplot(aes(x = sepal_width, y = mean, ymin = lower, ymax = upper,\n\t\t\t   color = species, fill = species, shape = species)) +\n\t# This part plots the mean line and CI\n\tgeom_ribbon(alpha = 0.25, color = NA) +\n\tgeom_line(size = 1) +\n\t# This part plots the raw data points\n\tgeom_point(\n\t\tdata = iris,\n\t\taes(x = Sepal.Width, y = Petal.Length, color = Species,\n\t\t\tshape = Species),\n\t\tinherit.aes = FALSE, size = 2, stroke = 1\n\t) +\n\t# This plot customizes how the plot looks\n\tscale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n\tscale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n\tscale_shape_manual(values = c(21, 22, 24)) +\n\tcoord_cartesian(ylim = c(0, 7)) +\n\tlabs(x = \"\\nsepal width\", y = \"petal length\\n\") +\n\tzlib::theme_ms()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nNote that this model is a so-called \"parallel slopes\" model. We estimated one common slope, with a different intercept for each group. If we also wanted the slope to vary, we would have needed to make the slope a random effect as well. (And probably specified the correlation between the slope and intercept.)\n\nHowever, you can see that we get individual credible intervals for each group, as we desired.\n\n### \"Type Three\" -- Typical new individual means\n\n### \"Type Four\" -- Similar new individual means\n\n## Using `brms` for double checking or whatever\n\n## The Wrap-Up\n\n## Details {.appendix}\n\nLast updated: 2022-10-04 15:25:02\n\n[source code](https://github.com/wzbillings/zlog/tree/master/_posts/posts/2022-08-22_brms-CIs/index.qmd)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19043)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] parallel  stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n[1] digest_0.6.29        rethinking_2.21      cmdstanr_0.5.3      \n[4] rstan_2.21.7         StanHeaders_2.21.0-7 ggplot2_3.3.6       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           mvtnorm_1.1-3        lattice_0.20-45     \n [4] tidyr_1.2.1          prettyunits_1.1.1    ps_1.7.1            \n [7] assertthat_0.2.1     utf8_1.2.2           R6_2.5.1            \n[10] backports_1.4.1      stats4_4.2.1         evaluate_0.16       \n[13] coda_0.19-4          highr_0.9            pillar_1.8.1        \n[16] rlang_1.0.5          data.table_1.14.2    rstudioapi_0.14     \n[19] callr_3.7.2          checkmate_2.1.0      emoji_0.2.0         \n[22] rmarkdown_2.16       labeling_0.4.2       stringr_1.4.1       \n[25] htmlwidgets_1.5.4    loo_2.5.1            munsell_0.5.0       \n[28] compiler_4.2.1       xfun_0.32            pkgconfig_2.0.3     \n[31] pkgbuild_1.3.1       shape_1.4.6          zlib_0.0.1          \n[34] htmltools_0.5.3      tidyselect_1.1.2     tibble_3.1.8        \n[37] tensorA_0.36.2       gridExtra_2.3        codetools_0.2-18    \n[40] matrixStats_0.62.0   fansi_1.0.3          crayon_1.5.2        \n[43] dplyr_1.0.10         withr_2.5.0          MASS_7.3-57         \n[46] grid_4.2.1           distributional_0.3.1 jsonlite_1.8.0      \n[49] gtable_0.3.1         lifecycle_1.0.2      DBI_1.1.3           \n[52] magrittr_2.0.3       posterior_1.3.1      scales_1.2.1        \n[55] RcppParallel_5.1.5   cli_3.4.1            stringi_1.7.8       \n[58] farver_2.1.1         renv_0.15.5          ellipsis_0.3.2      \n[61] generics_0.1.3       vctrs_0.4.2          tools_4.2.1         \n[64] glue_1.6.2           purrr_0.3.4          processx_3.7.0      \n[67] abind_1.4-5          fastmap_1.1.0        yaml_2.3.5          \n[70] inline_0.3.19        colorspace_2.0-3     knitr_1.40          \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}