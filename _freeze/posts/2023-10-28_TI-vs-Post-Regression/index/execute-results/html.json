{
  "hash": "45c07628b4979b5c16c2288a78754fcf",
  "result": {
    "markdown": "---\ntitle: \"Increase vs post titer as regression outcome\"\nauthor: \"Zane Billings\"\ndate: \"2023-10-28\"\ndescription: |\n  Lots of people debate about whether titer increase (log of fold change) or\n  raw post-vaccination titer should be used as the outcome for a regression\n  model of immunological data. Under certain generative models however, these\n  models are essentially equivalent and we can show how.\nlicense: \"CC BY-SA\"\ndraft: false\n---\n\n\n\n\nFor a vaccine study with an immunogenicity endpoint, two commonly used\noutcomes are the raw post-vaccination titer, or the fold change in titers\nbetween the post-vaccination and pre-vaccination endpoint. Both provide\ninteresting information. However, under many of the simple statistical models\nthat are used for both outcomes, the model results are deterministically\nrelated, which I want to show here.\n\n## Generative model\n\nFirst, we generate an example dataset. This dataset will have two columns, just\npre-titer and post-titer with no other confounders. Here's the population\nparameters. Throughout this example, I will assume all effects for the titer\nvalues occur on the log scale for simplicity. Though we could have a lot\nof arguments about the generative model, I specifically chose a very simple\nand easy to work with generative model that I think illustrates the point that\nI want to make.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_parms <- list(\n\tS <- 32482304,\n\tN <- 1000,\n\talpha <- 3,\n\tbeta <- 0.5,\n\tpre_mean <- 2,\n\tpre_var <- 2,\n\terror_var <- 2\n)\nstr(sim_parms)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 7\n $ : num 32482304\n $ : num 1000\n $ : num 3\n $ : num 0.5\n $ : num 2\n $ : num 2\n $ : num 2\n```\n:::\n:::\n\n\nNow we generate the data. Even though HAI titers always have the $\\times 5$\nin the formulas, I left that off here for simplicity. Since it's a constant\nmultiplier it doesn't affect what's going on here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngen_data <- function(S, N, alpha, beta, pre_mean, pre_var, error_var) {\n\tset.seed(S)\n\tdat <- tibble::tibble(\n\t\tlog_pre = rnorm(N, pre_mean, pre_var),\n\t\tpre = (2 ^ log_pre),\n\t\tnoise = rnorm(N, 0, error_var),\n\t\tlog_post = alpha + beta * log_pre + noise,\n\t\tpost = (2 ^ log_post),\n\t\tTI = log_post - log_pre,\n\t\tFC = 2 ^ TI\n\t)\n\treturn(dat)\n}\nsim_dat <- do.call(gen_data, sim_parms)\nprint(sim_dat, n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 7\n  log_pre     pre  noise log_post  post    TI     FC\n    <dbl>   <dbl>  <dbl>    <dbl> <dbl> <dbl>  <dbl>\n1   6.95  124.    -1.72      4.75  27.0 -2.20  0.218\n2  -0.501   0.707  1.83      4.58  23.9  5.08 33.8  \n3   2.02    4.04   0.706     4.71  26.2  2.70  6.49 \n4   0.832   1.78   1.82      5.23  37.6  4.40 21.1  \n5   5.91   60.0   -1.23      4.72  26.4 -1.19  0.439\n# … with 995 more rows\n```\n:::\n:::\n\n\nIf we plot the pre-titers vs the post-titers, we see a cloud of points\naround the regression line, as we would expect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_dat) +\n\taes(x = pre, y = post) +\n\tgeom_point() +\n\tgeom_abline(slope = beta, intercept = alpha, color = \"red\") +\n\tscale_x_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2)) +\n\tscale_y_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe relationship between titer increase and pretiter is also completely\ndetermined by this linear model. For the specific set of parameters I chose\nfor the simulation, we see a negative effect of pre-titer on titer increase\n(which we think is true in real life), but with this generative model, this\nis entirely dependent on the value of $\\beta$. Because I set a value of $\\beta$\nwhich is in $(0, 1)$, we observe a positive correlation between pre and post-\ntiter, and a negative correlation between pre-titer and titer increase. We\nwill see why shortly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_dat) +\n\taes(x = pre, y = FC) +\n\tgeom_point() +\n\tscale_x_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2)) +\n\tscale_y_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Regression offsets\n\nWe can\nderive the regression line for the case where the titer increase is the outcome\nof interest.\n\n$$\n\\begin{aligned}\n\\log_2(\\text{post}) &= \\alpha + \\beta \\cdot \\log_2(\\text{pre}) \\\\\n\\log_2(\\text{post}) - \\log_2(\\text{pre}) &= \\alpha + \\beta \\cdot \\log_2(\\text{pre}) - \\log_2(\\text{pre})\\\\\n\\text{titer increase} &= \\alpha + (\\beta - 1) \\cdot \\log_2 (\\text{pre})\n\\end{aligned}\n$$\nThat is, we can include **the negative pre-titer as an offset term in the\nlinear model for titer increase to recover the same coefficients as we\nwould in the model for post-titer.** This is why we got the specific\nqualitative behavior we observed earlier.\n\nNote that a regression offset is a term that is included on the right-hand\nside of a linear model, but the coefficient is pre-specified as 1, and not\nestimated as part of the model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(log_post ~ log_pre, data = sim_dat)\nmod2 <- lm(TI ~ log_pre, data = sim_dat)\nmod3 <- lm(TI ~ log_pre, data = sim_dat, offset = (-1 * log_pre))\n```\n:::\n\n\nHere we can see that model 1 and model 3 recover the parameters from the\ngenerative model, while if we fit model 2, we get a biased coefficient that\nis exactly equal to 1 minus the estimated parameter from the other two models.\nSo while both models produce a \"correct\" inference, we need to be aware why\nthe models are different.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::bind_rows(\n\t\"mod1\" = coef(mod1),\n\t\"mod2\" = coef(mod2),\n\t\"mod3\" = coef(mod3),\n\t.id = \"model\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  model `(Intercept)` log_pre\n  <chr>         <dbl>   <dbl>\n1 mod1           3.05   0.479\n2 mod2           3.05  -0.521\n3 mod3           3.05   0.479\n```\n:::\n:::\n\n\nNote that adding **linear confounders will not change what happens here.** For\nexample, we could add $\\beta_2 \\cdot \\mathrm{age}$ into our generative model and\nour regression models, and we would see the same pattern. However, if we\nincorporate nonlinearity (e.g. interaction effects or hierarchical effects),\nunderstanding how the models are related might not be that simple to solve,\nbecause in those cases we can't necessarily combine the offset term with\nsomething else to simplify the model.\n\n## Other outcomes\n\nIf we were to construct an outcome that is not\nlinear in the pre-titer value, this equivalence would no longer hold. For\nexample, if instead of using the log fold change as the outcome, we instead\nused\n\n$$y = \\frac{\\log_2 (\\text{post})}{\\log_2 (\\text{pre})},$$\nthe models are no longer equivalent in this way. This model is not commonly\nused because the ratio of the logarithms is not as easily interpreted as the\nlog fold change, but we could do that if we wanted. If we fit a model with\nthat outcome we would get\n$$\n\\begin{aligned}\n\\frac{\\log_2 (\\text{post})}{\\log_2 (\\text{pre})} &=\n\\frac{\\alpha + \\beta\\cdot\\log_2 (\\text{pre})}{\\log_2 (\\text{pre})} \\\\\n&=\\beta + \\alpha\\frac{1}{\\log_2 (\\text{pre})}.\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1 / sim_dat$log_pre\ny <- sim_dat$log_post / sim_dat$log_pre\nmod4 <- lm(y ~ sim_dat$log_pre)\nmod5 <- lm(y ~ x)\ndplyr::bind_rows(\n\t\"mod4\" = coef(mod4),\n\t\"mod5\" = coef(mod5),\n\t.id = \"model\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  model `(Intercept)` `sim_dat$log_pre`      x\n  <chr>         <dbl>             <dbl>  <dbl>\n1 mod4          1.30              0.177 NA    \n2 mod5          0.899            NA      0.916\n```\n:::\n:::\n\n\nUh-oh, those aren't the numbers I said we should get! Actually, a major\nproblem with this model is that those transformations kind of wreck our\nnumeric precision. If we plot the transformed data, we can see that\nregression line still passes through the middle of the data really well,\nwe just get some crazy points that ruin our ability to estimate this with a\nplain linear regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y)\nabline(a = beta, b = alpha)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nSo I guess that's probably another reason this outcome doesn't get used much.\n\n## Details {.appendix}\n\nLast updated at 2023-10-28 22:18:34.847873.\n\n[source code](https://github.com/wzbillings/quarto-website/blob/main/posts/2023-10-28_TI-vs-Post-Regression)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.6\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.5.0       cli_3.4.1         knitr_1.40        rlang_1.0.6      \n [5] xfun_0.34         stringi_1.7.8     generics_0.1.3    renv_0.16.0      \n [9] jsonlite_1.8.3    glue_1.6.2        colorspace_2.0-3  htmltools_0.5.3  \n[13] fansi_1.0.3       scales_1.2.1      rmarkdown_2.17    grid_4.3.1       \n[17] evaluate_0.17     munsell_0.5.0     tibble_3.1.8      fastmap_1.1.0    \n[21] yaml_2.3.6        lifecycle_1.0.3   zlib_0.0.1        stringr_1.4.1    \n[25] compiler_4.3.1    dplyr_1.0.10      pkgconfig_2.0.3   htmlwidgets_1.5.4\n[29] farver_2.1.1      digest_0.6.30     R6_2.5.1          tidyselect_1.2.0 \n[33] utf8_1.2.2        pillar_1.8.1      magrittr_2.0.3    withr_2.5.0      \n[37] tools_4.3.1       gtable_0.3.1     \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}