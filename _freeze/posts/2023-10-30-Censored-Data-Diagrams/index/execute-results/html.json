{
  "hash": "a4c90e7e9ec3723bc75e316c9421beb6",
  "result": {
    "markdown": "---\ntitle: \"My data are censored!! What do I do??\"\nauthor: \"Zane Billings\"\ndate: \"2023-10-30\"\ndescription: |\n  I needed to make some censored data plots for a presentation, and it turned\n  out to be easier to write the material as a blog post first and then convert\n  that into a presentation afterwards.\nlicense: \"CC BY-SA\"\ndraft: true\n---\n\n\n\n\nRecently, I've been working a lot on censored data in the context of antibody\nmeasurements, which often have limits of detection or precision issues that\ncan be treated as censored data problems. I've been working with my\n[awesome lab group](https://handelgroup.uga.edu/) on a larger project about\napproaches for censored data analyses, but this blog post will cover the\nvery basics. I needed to make a presentation for a seminar about this topic,\nand it turned out it was easier to write the blog post first and then\nconvert it into a presentation.\n\nHere are the learning goals for this blog post, and the motivating issues for\nme to write about. If I've done a decent job, by the end of this post, a\ndiligent reader should be able to:\n\n1. Explain what censored data is, and give an example of how censoring might\noccur;\n1. List the types of censored data, and be able to identify these types in \npractical examples;\n1. Write down a data generating model for censored data, and write down the\nlikelihood for the censored variables;\n1. Explain how censored data are handled in survival, tobit-type, and Bayesian\nmodels using the likelihood;\n1. Identify pros and cons for frequentist and Bayesian approaches to handle\ncensored data.\n\n## What is censored data?\n\n**Censoring** is a type of data coarsening which occurs due to the way\ndata are observed. Such data are only partially known, i.e., we know a range\nof values for a given data point, but not the exact value of that\ndata point. The most common example in the literature occurs in survival\nanalysis. Notably, when data are censored, we still have a record for a given\nindividual, with an imprecise measurement -- we will not consider **truncated**\ndata here, where the measurement process completely excludes measurements\nwhich are not known precisely.\n\n## Censored survival data\n\nSurvival analysis methods are used to analyze data where the outcome is the time\nwhen a specific event occurs -- originally death, which is where the name comes\nfrom. Survival analysis is also called reliability analysis in fields where\ninstruments or machines are the subject of consideration rather than people. In\nepidemiology, these types of survival outcomes often come up in the context of a\n**cohort study**. We recruit a bunch of people, follow them for a specific\namount of time, and observe when certain outcomes happen. Importantly, *if the\nstudy ends before we observe an outcome for a given individual, that does not\nmean the individual never gets an outcome.*\n\nInstead, our observation of no outcome occurrence for that individual is a\nfeature of our observation process -- we would follow all individuals in our\nstudy until they were no longer at risk for the event if we could, but real\nworld constraints limit the amount of follow-up or observation time we can\nhave. The outcome observation for this person is said to be **right censored**,\nwhich is the classical type of censoring dealt with in epidemiology. The term\n\"right\" censoring comes from the fact that this individual's time-to-event\nis in the interval $(\\text{end of study}, \\ \\infty)$, so the true value could\nbe anywhere to the right of our observed measurement on a number line.\n\nOf course, a cohort study can be defined in calendar time or in event time, but\nfor the sake of simplicity we'll assume that our cohort is in calendar time.\nSay for example, that we started our study (and had all patients recruited) on\nJanuary 10, 2023 and completed all of our follow-up on\nMay 16, 2023.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rc) +\n\taes(x = study_start, color = censored, group = id) +\n\tgeom_vline(xintercept = study_start, color = \"gray\", lwd = 2) +\n\tgeom_vline(xintercept = study_end, color = \"gray\", lwd = 2) +\n\tgeom_segment(\n\t\taes(xend = event_time, y = id, yend = id),\n\t\tlwd = 1.5,\n\t\tshow.legend = FALSE\n\t) +\n\tgeom_point(\n\t\taes(x = study_start, y = id),\n\t\tsize = 3, stroke = 2, shape = 16,\n\t\tshow.legend = FALSE\n\t) +\n\tgeom_point(\n\t\taes(x = event_time, y = id, shape = censored),\n\t\tsize = 3, stroke = 2\n\t) +\n\tgeom_label(\n\t\taes(\n\t\t\t# Insane date math to get the graphical midpoint\n\t\t\tx = lubridate::as_date((\n\t\t\t\tpmax(as.numeric(study_start), as.numeric(study_start - 14 * 2)) +\n\t\t\t\t\tpmin(as.numeric(event_time), as.numeric(study_end + 14 * 1))\n\t\t\t) / 2),\n\t\t\tlabel = paste0(\"Participant #\", id),\n\t\t\ty = id + 0.25\n\t\t), fill = \"#ffffff\", col = \"black\"\n\t) +\n\tscale_color_manual(values = c(\"black\", \"red\"), name = NULL) +\n\tscale_x_date(\n\t\tname = \"Calendar time\",\n\t\texpand = expansion(mult = 0.025),\n\t\t#date_breaks = \"2 weeks\",\n\t\tbreaks = seq(study_start, study_end + 14, 14),\n\t\tdate_labels = \"%b %d\"\n\t) +\n\tcoord_cartesian(xlim = c(study_start, study_end + 14)) +\n\tscale_y_continuous(breaks = NULL, name = \"Participant\") +\n\tscale_shape_manual(values = c(16, 4), name = NULL)\n\nggsave(\n\there::here(\"posts\", pth_base, \"rc.png\"),\n\twidth = 16, height = 9\n)\n```\n\n::: {.cell-output-display}\n![Example study data which include right censored observations. For\nthis example, we assume that no one experienced the event before the study\nbegan, so only right censoring is possible. Individuals who did not\nexperience the event before the end of the study are right-censored, and\ntheir study times are colored red. Individuals who experienced the\nevent before the end of the study are not censored, and their study\ntimes are colored black. The points (circles for completely observed\ndata, x's for censored data) show the value we observe as part of the\nstudy.](index_files/figure-html/fig-rc-1.png){#fig-rc width=672}\n:::\n:::\n\n\nNote that here we have also implicitly assumed that the only reason for a\nperson to drop out of the study is because the event occurs. We could,\nin real life, have subjects who dropped out of the study before the event\noccurred, and in real life we would likely not have a fixed calendar date\nfor enrollment. But we make these simplifying assumptions for now, because\nthis kind of survival analysis is not the main point here, the idea of\ncensoring is the main point and survival analysis is a motivating example.\n\nThinking about censored data in this way is useful for analyzing survival data,\nbut can be misleading when trying to understand the general idea of censored\ndata. Such a study would produce a distribution of survival times which is\n**censored in the right tail.** With only 10 observations like we show in the\nprevious figure, visualizing the distribution of survival times might be\ndifficult. But if we draw survival times from some parametric model, and then\ncensor them, we can see this effect in practice. So suppose these are 10\nindividuals from a study that had 1000 individuals. The nature of the parametric\nmodel and how I made this distribution is not important but is explained in\nthe callout box for anyone curious.\n\n::: {.callout-note icon=false collapse=true appearance=\"simple\"}\n### Generating histogram data {.unnumbered}\n\n1. To create the data in the previous chart showing observation and event times\nfor each participant, I just made up dates until I got a plot that looked how\nI wanted it to.\n1. The outcome we're showing the distribution of here is the time-to-event,\nthat is, the (latent) event time minus the study start time. There are several\ntypical parametric distributions for time-to-event variables, so I randomly\nchose a common one called the Weibull distribution and used the function\n`MASS::fitdistr()` to obtain the maximum likelihood estimates for the Weibull\ndistribution matching my made up data. *We can then\nplot the density curve on the plot below using the parameters from that\ndistribution.*\n1. I then generated 990 independent random deviates from the Weibull\ndistribution with those estimated parameters, then concenate those to the\nvalues I made up. We now have a larger sample that best matches the numbers\nI made up randomly (under a specific parametric assumption).\n1. Apply censoring. That is, if a failure time is greater than the total time\nelapsed during the study, record it instead as the total time elapsed.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_time <- lubridate::interval(study_start, study_end) / lubridate::days(1)\nset.seed(12341234)\nrc_vals <- lubridate::interval(study_start, rc$event_time) / lubridate::days(1)\nest <- MASS::fitdistr(rc_vals, densfun = \"weibull\")\nsim_dist_rc <- tibble::tibble(\n\ty_star = c(\n\t\trc_vals,\n\t\trweibull(990, shape = est$estimate[[1]], scale = est$estimate[[2]])\n\t),\n\torig = c(rep(1, times = 10), rep(0, times = 990)),\n\tcens = ifelse(y_star <= total_time, 0, 1),\n\ty_cens = pmin(total_time, y_star)\n)\n\ndist_dat <- tibble::tibble(\n\tx = seq(0, 300, 0.01),\n\ty = dweibull(x, shape = est$estimate[[1]], scale = est$estimate[[2]])\n)\nggplot(sim_dist_rc) +\n\t#geom_vline(xintercept = total_time) +\n\tgeom_histogram(\n\t\taes(x = y_cens, y = after_stat(density)), binwidth = 5,\n\t\tboundary = 0,\n\t\tcolor = \"black\", fill = \"gray\"\n\t) +\n\tgeom_line(data = dist_dat, aes(x = x, y = y)) +\n\tlabs(x = \"Time-to-event (survival time)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rc-dist-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(\n\there::here(\"posts\", pth_base, \"rc-hist.png\"),\n\twidth = 16, height = 9\n)\n```\n:::\n\n\nHere, the histogram is showing the observed data from our censored sample.\nThere is a huge bin containing the censored values, which in this case are all\nsurvival times that were larger than the amount of time each individual spent\nin the study. The black curve shows the true density curve from which I\nsampled the data, so we can see that the censored data are censored at a\nspecific measurement. Basically all of the density that would be above the\ncensoring point gets pushed into the closest bin just below the censoring point.\nSo this is how we can understand the term \"right\" censoring.\n\nIf there is right censoring, you might be wondering, is there also left\ncensoring? The answer is yes, and it occurs in much the same way.\nIn my first draft of this post, I got into a detailed discussion of survival\nanalysis and censoring types here, but then realized it was not particularly\nhelpful. So I decided to leave that bit out.\n\n\n<!--\nIf we can also recruit patients who have **already experienced the event**,\nsuch subjects would have **left censored** survival times in our analysis. We\nknow that those subjects have a survival time that is less than or equal to\nthe amount of elapsed time since the study start and their recruitment into\nthe study. I think that was a really complicated way to explain that, so let's\nconsider an example.\n\nSuppose we are interested in conducting a large-scale study of risk factors for\ndementia in adults who are 65 or older. We will, for the purposes of this\nexample, exclude cases of early-onset dementia from our analysis, and thus\nassume that dementia does not occur until after age 65. We continuously\nrecruit new patients into our study with the only inclusion criteria being\nthat they are at least 65 and visit a clinical site which is part of our\nstudy. Thus, we could recruit a patient who is age 70 and has already been\ndiagnosed with dementia. Said participant's survival time is therefore\nleft-censored: we know it is less than 5, but not the actual value. (Note\nhere that survival times **must** be greater than zero. In a future section\nwe will see examples without a natural lower bound.)\n\nSuch a cohort might have a diagram that looks something like this.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(lc) +\n\taes(x = t2, y = id, color = censored, group = id) +\n\tgeom_vline(xintercept = study_start, color = \"gray\", lwd = 2) +\n\tgeom_vline(xintercept = study_end, color = \"gray\", lwd = 2) +\n\tgeom_segment(\n\t\taes(xend = t1, y = id, yend = id),\n\t\tlwd = 1.5,\n\t\tshow.legend = FALSE\n\t) +\n\t# geom_segment(\n\t# \taes(\n\t# \t\tx = t1, xend = t1,\n\t# \t\ty = id - 0.5, yend = id + 0.5\n\t# \t),\n\t# \tlwd = 1.5,\n\t# \tshow.legend = FALSE\n\t# ) +\n\tgeom_label(\n\t\taes(\n\t\t\t# Insane date math to get the graphical midpoint\n\t\t\tx = lubridate::as_date((\n\t\t\t\tpmax(as.numeric(t1), as.numeric(study_start - 14 * 2)) +\n\t\t\t\t\tpmin(as.numeric(t2), as.numeric(study_end + 14 * 1))\n\t\t\t) / 2),\n\t\t\tlabel = paste0(\"Participant #\", id)\n\t\t)\n\t) +\n\tgeom_point(\n\t\taes(x = t2, y = id, shape = r_censored),\n\t\tsize = 3, stroke = 2\n\t) +\n\tgeom_point(\n\t\taes(x = t1, y = id, shape = l_censored),\n\t\tsize = 3, stroke = 2\n\t) +\n\t# annotate(\n\t# \t\"label\",\n\t# \tx = study_start,\n\t# \ty = 10.75,\n\t# \tlabel = \"Study start\"\n\t# ) +\n\t# annotate(\n\t# \t\"label\",\n\t# \tx = study_end,\n\t# \ty = 10.75,\n\t# \tlabel = \"Study end\"\n\t# ) +\n\tscale_color_manual(values = c(\"black\", \"red\"), name = NULL) +\n\tscale_x_date(\n\t\tname = \"Calendar time\",\n\t\texpand = expansion(mult = 0.025),\n\t\t#date_breaks = \"2 weeks\",\n\t\tbreaks = seq(study_start - 14 * 1, study_end + 14 * 1, 14),\n\t\tdate_labels = \"%b %d\"\n\t) +\n\tcoord_cartesian(xlim = c(study_start - 14 * 1, study_end + 14 * 1)) +\n\tscale_y_continuous(breaks = NULL, name = \"Participant\") +\n\tscale_shape_manual(values = c(16, 4), name = NULL)\n```\n\n::: {.cell-output-display}\n![The caption](index_files/figure-html/fig-lc-1.png){#fig-lc width=672}\n:::\n:::\n\n\nImportantly, in this type of study, we **cannot define our study time-scale\nin calendar time -- it doesn't make sense any more, because then our left\ncensored individuals would have negative time-to-event.** In the dementia\nstudy example, we would define the survival time as the amount of time between\nthe participant's 65th birthday and the onset of dementia.\n\nLeaving aside that example (so I don't have to simulate all that stuff), we can visualize what a left-censored distribution\nmight look like. Suppose we use the same parametric model as the previous\nhistogram, but this time we have as much follow-up time as we want, but we\ndon't observe any values \n\n-->\n\nInstead, let's focus on the more general idea of censored data, and censored\ndata types.\n\n## Types of censored data\n\nLet's start by writing down the **data generating process** for our\nfamiliar survival analysis model. This will hopefully be instructive for the\ntypes of censoring we'll list momentarily. Recall we assume that the survival\ntime, let's say $t_i$, where $i$ stands for a given individual and counts up\nfrom $1$ to $n$, the sample size, has some parametric model. The formula\nfor the model doesn't matter, so we'll call it $f(t)$ to be general. However,\nwe don't actually observe the value $t_i$, we observe the censored value, say\n$c_i$ instead. The value $c_i$ is equal to $t_i$, if it is less than the\namount of elapased time in our study, say $t_{\\text{max}}$, otherwise it\nis equal to $t_{\\text{max}}$. So in math language, we would write the DGP as\n$$\n\\begin{aligned}\nt_i &\\sim f(t) \\\\\nc_i &= \\min\\left(t_i, t_{\\max} \\right).\n\\end{aligned}\n$$\nIf we wanted to use that Weibull distribution from before as our parametric\nmodel, we would write\n$$\n\\begin{aligned}\nt_i &\\sim \\mathrm{Weibull}\\left(\\theta\\right) \\\\\nc_i &= \\min\\left(t_i, t_{\\max} \\right)\n\\end{aligned}\n$$\nwhere $\\theta$ is a vector of parameters. Remember that this is called\n**right-censored data**. We can define a few other types of censoring by\nanalogy.\n\n* **Right censoring**: the censored values are less than or equal to some\nmaximum value.\n* **Left censoring**: the censored values are greater than or equal to some\nminimum value.\n* **Interval censoring**: the censored values are known to be in some\ninterval, but we do not know where in that interval.\n\nLeft censoring can be understood mostly by analogy to right censoring. For\nleft censored data, we would just say\n$$c_i = \\max (t_i, t_{\\min})$$\ninstead of what we wrote before. Interval censoring is somewhat more tricky\nhowever, although it occurs in a good number of circumstances.\n\nTo understand interval censoring, let's talk about an example. Suppose we\nrecruit a cohort of people who do not have HIV. If our outcome is time until\ndiagnosis of HIV, we have to regularly test those people for HIV. Suppose\nwe ask them to come get a test once per month because we don't have enough\nfunding to do it any more frequently. Once an individual tests positive, we\ncould say their survival time was $X$ months, but this is not really true --\nwe only know that their survival time is somewhere between this month and\nthe previous month, so this outcome is **interval censored**.\n\nOf course there are a lot more complications too. Here's an incomplete list\nof types of censoring that you might see referenced in the wild with a\nshort explanation.\n\n* **Singly censored**: the censoring point or threshold is a fixed, constant\nvalue, and is the same for all observations. All of the examples we've\ndiscussed so far are singly censored.\n* **Multiply censored**: Censoring times vary across observations. This is\ncommon in chemical or environmental studies where concentration analysis\nwhere the threshold can change across instruments or batches.\n* **Doubly censored**: values have both an upper and a lower threshold.\nConfusingly, this is not related to the idea of singly vs. multiply censored\ndata. In this case, the observed censored value is given by\n$c_i = \\max \\left\\{ \\min \\left\\{t_i, t_{\\max}\\right\\}, t_{\\min} \\right\\}.$\nMany lab measurements are doubly censored due to equipment limits. Note that\nthis is distinct from interval censoring though they seem similar; additionally,\na given observation can only be left censored or right censored, but a variable\ncan be doubly censored.\n* **Type I censoring**: also called \"time-terminated\" censoring, when the study\nends at a specific point in time. Almost all epidemiologic studies fall into this\ncategory.\n* **Type II censoring**: also called \"failure-\" or \"event-terminated\" censoring,\nwhen the study ends after a pre-specified number of failures. Such a study\ndesign can help with statistical power, but is impractical to run.\n* **Progressive censoring**: There is a sequence of censoring times (generally\ncalled terminals) $T_1 < T_2 < \\ldots < T_j \\ldots < T_k$. At the value\n$T_j$, there are a total of $c_j$ censored observations of the variable $x$\nwhere we know only that $x > T_j$. Given a sample size $N$ and a number of\nfully observed measurements $n$, we have that $N = n + \\sum_{j=1}^k c_j$. This\ncensoring scheme is rare and complicated, and we will not discuss it further.\n\nNow, given that laundry list, we can get back to the interesting discussion.\n\n## Censored non-survival data\n\nSo far, we have only discussed right-censored survival data. But censoring\nis part of the observation process, and can occur in arbitrarily complex ways\non data with arbitrary generating processes. See for example the 2013 Polish\n*matura* exam scores in @fig-matura. The data in this figure are available from\na [2014 reddit thread](https://web.archive.org/web/20140608024521/https://www.reddit.com/r/dataisbeautiful/comments/27dx4q/distribution_of_results_of_the_matura_high_school/) and from the [original source, in Polish](https://web.archive.org/web/20141213135754/http://www.cke.edu.pl/images/files/matura/informacje_o_wynikach/2013/2013_Matura.pdf) (p. 18).\n\n![Polish matura exam scores in 2013.](matura.png){#fig-matura}\n\nI guess you can tell what the passing grade for the exam was!\nWe have a definite case of censoring here, although it is more complicated\nthan our previous case. Here, it looks like there is some probability, based\non the exam score and the grader, that a grade in the range $\\{23, \\ldots, 29\\}$\ngot bumped up to a 30 (the minimum passing score) or 31. This is a censoring\nprocess with both a systematic and a random element that would probably\nbe quite hard to model without a lot of data. Although I need to remind myself\nin the future\nthat doing a little demo of a DGP to get data that looks like that might\nbe kind of fun!\n\nExtremely complicated censoring processes aside, **censored non-survival data\nare quite common**! We don't hear about them often in epidemiology, because\nunfortunately as a field we are very likely to simply ignore censoring in these\nvalues and pretend it doesn't exist. For example, I work with a lot of antibody\ndata in my research. Antibody data, and most immunological data for that matter,\ntends to feature a **lower limit of detection** -- what we called\n$t_{\\min}$ earlier. Instead of observing the true underlying value, we observe\n$\\max\\{t_i, t_{\\min}\\}$, giving us **left-censored data**! And of course\nantibody titers are not survival times, and do not follow the same distributions.\n\nIn fact, the most common distributional assumptions for antibody data are\nlog-normal or gamma. Now, these can be used for parametric survival models\nunder certain assumptions. And we are lucky that most survival models, such\nas the ones implemented in `R`, include those distributions in what they can\nfit. And we are even luckier that if $x$ is a left-censored variable, $-x$ is\na right-censored variable, so we can just do a survival model with the correct\ndistribution where the outcome is $-x$.\n\n## An aside on the likelihood\n\n## MLE on censored data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(134123)\nx_star <- rnorm(10000, 0, 1)\nx <- ifelse(x_star < 0, 0, x_star)\n\nstar_dens <- density(x_star, kernel = \"gaussian\")\nx_dens <- density(x, kernel = \"gaussian\", bw = star_dens$bw)\n\nplot(\n\tNULL, NULL,\n\txlim = c(min(star_dens$x, x_dens$x), max(star_dens$x, x_dens$x)),\n\tylim = c(min(star_dens$y, x_dens$y), max(star_dens$y, x_dens$y)),\n\txlab = \"x\", ylab = \"density\"\n)\nhist(x_star, breaks = seq(-5, 5, 0.1), freq = FALSE, add = TRUE,\n\t col = \"#E69F0060\")\nhist(x, breaks = seq(-5, 5, 0.1), freq = FALSE, add = TRUE,\n\t col = \"#56B4E960\")\nlines(star_dens, lty = 1, col = \"orange\", lwd = 2)\nlines(x_dens, lty = 2, col = \"blue\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## The Bayesian alternative\n\n## Conclusion\n\n## Details {.appendix}\n\nLast updated at 2023-11-06 09:05:47.902596.\n\n[source code](https://github.com/wzbillings/quarto-website/blob/main/posts/2023-10-30-Censored-Data-Diagrams)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] lubridate_1.9.3 ggplot2_3.4.4  \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.1      jsonlite_1.8.7    dplyr_1.0.10      compiler_4.3.1   \n [5] renv_1.0.3        tidyselect_1.2.0  stringr_1.4.1     scales_1.2.1     \n [9] yaml_2.3.6        fastmap_1.1.0     here_1.0.1        R6_2.5.1         \n[13] labeling_0.4.2    generics_0.1.3    knitr_1.40        MASS_7.3-58.2    \n[17] htmlwidgets_1.5.4 tibble_3.1.8      rprojroot_2.0.3   munsell_0.5.0    \n[21] pillar_1.8.1      rlang_1.1.1       utf8_1.2.2        stringi_1.7.8    \n[25] xfun_0.34         timechange_0.2.0  cli_3.6.1         withr_2.5.0      \n[29] magrittr_2.0.3    digest_0.6.33     grid_4.3.1        zlib_0.0.1       \n[33] lifecycle_1.0.3   vctrs_0.5.0       evaluate_0.23     glue_1.6.2       \n[37] farver_2.1.1      fansi_1.0.3       colorspace_2.0-3  rmarkdown_2.17   \n[41] tools_4.3.1       pkgconfig_2.0.3   htmltools_0.5.3  \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}