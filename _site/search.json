[
  {
    "objectID": "posts/2022-12-17_histogram-bins/index.html",
    "href": "posts/2022-12-17_histogram-bins/index.html",
    "title": "Histogram binwidths",
    "section": "",
    "text": "An example of a histogram."
  },
  {
    "objectID": "posts/2022-12-17_histogram-bins/index.html#last-updated",
    "href": "posts/2022-12-17_histogram-bins/index.html#last-updated",
    "title": "Histogram binwidths",
    "section": "Last updated",
    "text": "Last updated\n\n\nSys.time()\n\n[1] \"2022-12-17 12:16:52 EST\""
  },
  {
    "objectID": "posts/2022-12-17_histogram-bins/index.html#details",
    "href": "posts/2022-12-17_histogram-bins/index.html#details",
    "title": "Histogram binwidths",
    "section": "Details",
    "text": "Details\n\nsource code\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.6\n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.2    tools_4.2.2       digest_0.6.30    \n [5] jsonlite_1.8.3    evaluate_0.17     lifecycle_1.0.3   tibble_3.1.8     \n [9] gtable_0.3.1      pkgconfig_2.0.3   rlang_1.0.6       cli_3.4.1        \n[13] yaml_2.3.6        xfun_0.34         fastmap_1.1.0     withr_2.5.0      \n[17] stringr_1.4.1     dplyr_1.0.10      knitr_1.40        generics_0.1.3   \n[21] htmlwidgets_1.5.4 vctrs_0.5.0       cowplot_1.1.1     grid_4.2.2       \n[25] tidyselect_1.2.0  glue_1.6.2        R6_2.5.1          fansi_1.0.3      \n[29] rmarkdown_2.17    farver_2.1.1      magrittr_2.0.3    scales_1.2.1     \n[33] htmltools_0.5.3   colorspace_2.0-3  renv_0.16.0       labeling_0.4.2   \n[37] utf8_1.2.2        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "",
    "text": "Show the code\nlibrary(ggplot2)\nbase_pth &lt;- here::here(\"posts/2024-09-22-Great-Weapon-Fighting\")\nsource(here::here(base_pth, \"enumeration.R\"), echo = FALSE)\nThe new version of the Dungeons and Dragons 5th Edition rules came out this month, featuring some changes to features from the 2014 version of the rules. I’m getting ready to start a new campaign (as a player! which is still very fun and unusual for me) and I decided that I’m going to go out on a limb and play a fighter this time. And of course, if I’m going to be a fighter, I’m going to have a Cloud Strife, Zabuza, Guts Berserk type of sword.\nAn interesting statistical exercise arises from the fact that Great Weapon Fighting, an ability that makes your fighter character better at using large weapons, is one of the abilities that has been updated in the 2024 edition of the Player’s Handbook. While the change in wording might seem small from just reading the text, the damage distributions of affected weapons is changed a lot. For reference, here’s the text from the 2014 version.\nAnd here’s the text from the 2024 version.\nIn the 2014 version, you can still roll a 1 or 2, although the chance is greatly diminished. In the 2024 version, the lowest number you can roll is a 3. Now, when I first read this, my thought was “oh, clearly the 2024 version is better” which I think is an easy misinterpretation to make. While the threshold for the lowest value you can roll is raised, you are actually less likely to get high rolls with the 2024 version than with the 2014 version. That’s because those rerolls are powerful – instead of just increasing the probability of a 3, they increase the probability of rolling values that are higher than three.\nOf course, as a statistical something-or-other, my inclination was to quantify how influential this effect as. And as a D&D player since 2016-ish (hard to believe it’s been 8 years…), I thought this would be a pretty fun problem to solve. It’s also not too difficult, fortunately. So let’s walk through the solution.\nOne major caveat is that which of the versions is better depends on the die we’re rolling. For example, if we’re rolling four-sided dice (d4’s in D&D nomenclature), bumping our lower values up to three will have a larger impact on the expected value of our rolls than if we’re rolling a d12. So I’ll work this problem out for the two most common types of damage rolls for a GWF-affected heavy weapon in 5th edition D&D: a weapon that does 1d12 damage, and a weapon that does 2d6 damage. I have, for some time, been a 2d6 fan (because rolling more dice is more fun) and it turns out the resulting distributions in the 2d6 case are more interesting, so that’s yet another reason to make a character using a greatsword, which is one of those 2d6 weapons."
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html#no-gwf",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html#no-gwf",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "No GWF",
    "text": "No GWF\nThe case for no GWF is simple. For the 1d12 weapon, assuming the die is fair, the distribution is just \\(1/12\\) for each of the values. The distribution for 2d6 is not much more complicated, and is a common problem in introductory probability courses and textbooks. I’ll use a set of helper functions that will be useful for more complex problems that I included in the enumeration code file.\nHere’s the table of probabilities for a 2d6 weapon with no GWF.\n\n\nShow the code\nget_distribution(6, 2, \"none\", FALSE) |&gt;\n  knitr::kable()\n\n\n\n\n\nvalue\nfreq\nrel_freq\n\n\n\n\n2\n1\n0.0278\n\n\n3\n2\n0.0556\n\n\n4\n3\n0.0833\n\n\n5\n4\n0.1111\n\n\n6\n5\n0.1389\n\n\n7\n6\n0.1667\n\n\n8\n5\n0.1389\n\n\n9\n4\n0.1111\n\n\n10\n3\n0.0833\n\n\n11\n2\n0.0556\n\n\n12\n1\n0.0278\n\n\n\n\n\nUsing the normal rules for calculating the mean and standard deviation from this kind of distribution table, we can get that in summary:\n\nthe 1d12 weapon will have an expected value of \\(6.50 ± 3.45\\); and\nthe 2d6 weapon will have an expected value of \\(7.00 ± 2.42\\).\n\nOf course these summary metrics are not the only useful information for us, but they are a concise way to represent the amount of damage we can expect to do with one attack."
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html#gwf",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html#gwf",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "2024 GWF",
    "text": "2024 GWF\n\nGreat Weapon Fighting. When you roll damage for an attack you make with a Melee weapon that you are holding with two hands, you can treat any 1 or 2 on a damage die as a 3. The weapon must have the Two-Handed or Versatile property to gain this benefit. — Player’s Handbook, 2024.\n\nI’m doing the 2024 version first because it’s actually less interesting than the 2014 version. To compute the probabilities for one die, we can calculate all the same probabilities from the no GWF case, and then compute the probability of rolling a three as the probability of rolling a three or less. That is, \\[\nP(3 \\mid \\text{2024 GWF}) = P(1 \\mid \\text{no GWF}) +\nP(2 \\mid \\text{no GWF}) +\nP(3 \\mid \\text{no GWF}).\n\\]\nSo for a 1d12 weapon, the probability of rolling a 3 would be \\(3/12 = 1/4\\), the probability of rolling a 1 or 2 is \\(0\\), and the probability of rolling any other number \\(4, 5, \\ldots, 12\\) is \\(1/12\\) as before.\n\n\nShow the code\nget_distribution(12, 1, \"new\", FALSE) |&gt;\n  knitr::kable()\n\n\n\n\n\nvalue\nfreq\nrel_freq\n\n\n\n\n3\n3\n0.2500\n\n\n4\n1\n0.0833\n\n\n5\n1\n0.0833\n\n\n6\n1\n0.0833\n\n\n7\n1\n0.0833\n\n\n8\n1\n0.0833\n\n\n9\n1\n0.0833\n\n\n10\n1\n0.0833\n\n\n11\n1\n0.0833\n\n\n12\n1\n0.0833\n\n\n\n\n\nFor a 2d6 weapon, we have the additional issue of having to add two dice together. So while the same formula works for one dice, like so:\n\n\nShow the code\nget_distribution(6, 1, \"new\", FALSE) |&gt;\n  knitr::kable()\n\n\n\n\n\nvalue\nfreq\nrel_freq\n\n\n\n\n3\n3\n0.5000\n\n\n4\n1\n0.1667\n\n\n5\n1\n0.1667\n\n\n6\n1\n0.1667\n\n\n\n\n\nwe need to calculate the distribution of the sum. The function that I’m using does this by enumerating all of the possible combinations of the two die rolls, calculating the sum of each combination, and normalizing to get the percentages. For the 2024 version of GWF, we can get the combinations in the correct proportions by finding the combinations of \\(k\\) dice with the faces \\(3, 3, 3, 4, 5, \\ldots, n\\) instead of \\(1, 2, \\ldots, n\\). This tactic of using one die with modified faces will be crucial for the 2014 GWF probabilities so it’s nice to think about this problem that way.\nAnyways, calculating the probabilities in that way gives the following table for a 2d6 weapon.\n\n\nShow the code\nget_distribution(6, 2, \"new\", FALSE) |&gt;\n  knitr::kable()\n\n\n\n\n\nvalue\nfreq\nrel_freq\n\n\n\n\n6\n9\n0.2500\n\n\n7\n6\n0.1667\n\n\n8\n7\n0.1944\n\n\n9\n8\n0.2222\n\n\n10\n3\n0.0833\n\n\n11\n2\n0.0556\n\n\n12\n1\n0.0278\n\n\n\n\n\nIt makes sense that 6 is the lowest value we can roll – each of the two dice has to be a 3 or greater. The dip in probability for 7 is interesting though, because 7 is famously the most common number to roll for 2d6 without any special rules. This distribution also forms an interesting asymmetrical shape with a weird dip in it.\nIn summary: for the 2024 GWF version, we get\n\nthe 1d12 weapon will have an expected value of \\(6.75 ± 3.11\\); and\nthe 2d6 weapon will have an expected value of \\(8.00 ± 1.63\\).\n\nInterestingtly, the expected improvement is much higher for the 2d6 weapon than for the 1d12 weapon. That’s a combination of the effect of improving two die rolls instead of just one, and the fact that the new correction is better for weapons with smaller damage dice. So maybe a weapon that uses a sum of d4s (or d3s? lol) would be best with this correction. Like a greatwhip or something else that doesn’t exist."
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html#gwf-1",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html#gwf-1",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "2014 GWF",
    "text": "2014 GWF\n\nGreat Weapon Fighting. When you roll a 1 or 2 on a damage die for an attack you make with a melee weapon that you are wielding with two hands, you can reroll the die and must use the new roll, even if the new roll is a 1 or a 2. The weapon must have the two-handed or versatile property for you to gain this benefit. — Player’s Handbook, 2014.\n\nThe 2014 GWF case is the hardest one to calculate analytically. The trick to this one is to stop thinking about the problem as “one die roll, and sometimes a second one”. Thinking about it that way will give you results, but it is conceptually more difficult. Instead we should try and reframe the problem as “if I were only rolling one die, what faces would that die have to have?” The die for this problem will certainly not exist in real life, so first let’s take the 1d12 case as an example.\n\nWe can get a 1 by rolling either a 1 or 2, which will cause us to reroll, and then rolling a 1 on the second die. These are the ONLY ways we can roll a 1 for the result. So there are two ways we can get a 1.\nWe can get a 2 by rolling either a 1 or 2, triggering a reroll, then then rolling a 2. So similarly there are two ways we can get a 2 as our result.\nWe can get a 3 by rolling a 3 on the first die, or by rolling either a 1 or 2 on the first die, and then a 3 on the second die. The tricky part is changing how we think about this roll.\nWe can get a 4, 5, whatever, up to 12, in the exact same way as a 3, those numbers are all equally likely.\n\nTo think about the probability of rolling a 3, imaging that you always roll the second die. However, if the result of the first die is not a 1 or a 2, we just ignore the second die. Since we’re rolling 2d12, that means there are 144 possible outcomes, and if we were going to roll one hypothetical die, it would have to be 144 faces. (Or in general, \\(n^2\\) faces for an \\(n\\)-sided die.) From what we established above, there have to be two faces with a 1 on them, and then two faces with a 2 on them. Then we have 140 faces left to fill with 10 equally likely outcomes, so each remaining die has to get \\(14\\) faces.\nWe can also think about choosing the number of faces that show, e.g., a 3, like this. If we roll a three on the first d12, there are twelve ways we can roll the second d12 and the outcome will still be 3. However, if we roll a 1 or 2 on the first d12, and then a 3 on the second d12, that gives us 2 more ways to get a 3 overall. So we see we can get \\(12 + 2 = 14\\) (or in the general case, \\(n+2\\)) faces with a 3 on them. And the math works out the same for faces numbered \\(4, \\ldots, 12\\).\nThat means the probability of rolling a given number is given by the number of faces showing that number divided by the total number of faces. So that’s \\[P(1) = P(2) = 2 / 144; \\quad P(3) = \\ldots = P(12) = 14 / 144.\\]\nThe table is shown below.\n\n\nShow the code\nget_distribution(12, 1, \"old\", FALSE) |&gt;\n  knitr::kable()\n\n\n\n\n\nvalue\nfreq\nrel_freq\n\n\n\n\n1\n2\n0.0139\n\n\n2\n2\n0.0139\n\n\n3\n14\n0.0972\n\n\n4\n14\n0.0972\n\n\n5\n14\n0.0972\n\n\n6\n14\n0.0972\n\n\n7\n14\n0.0972\n\n\n8\n14\n0.0972\n\n\n9\n14\n0.0972\n\n\n10\n14\n0.0972\n\n\n11\n14\n0.0972\n\n\n12\n14\n0.0972\n\n\n\n\n\nNow, that’s for just one die. If we want to roll multiple dice, say 2d6, under the 2014 GWF rules, we are actually rolling two of those special \\(n^2\\) faced dies we just discovered. Then we can get all of the combinations of two of those special dice and find the distribution of their sum. That’s exactly what the get_distribution() function is doing to get the following table.\n\n\nShow the code\nget_distribution(6, 2, \"old\", FALSE) |&gt;\n  knitr::kable()\n\n\n\n\n\nvalue\nfreq\nrel_freq\n\n\n\n\n2\n4\n0.0031\n\n\n3\n8\n0.0062\n\n\n4\n36\n0.0278\n\n\n5\n64\n0.0494\n\n\n6\n128\n0.0988\n\n\n7\n192\n0.1481\n\n\n8\n224\n0.1728\n\n\n9\n256\n0.1975\n\n\n10\n192\n0.1481\n\n\n11\n128\n0.0988\n\n\n12\n64\n0.0494\n\n\n\n\n\nWe can see that now, 9 is the mostly likely outcome instead of 7, and the low rolls of 2 and 3 are much less likely as well.\nIn summary: for the 2014 GWF version, we get\n\nthe 1d12 weapon will have an expected value of \\(7.33 ± 3.00\\); and\nthe 2d6 weapon will have an expected value of \\(8.33 ± 2.01\\).\n\nSo again, it seems that the improvement favors the 2d6 weapon instead of the 1d12 weapon. The effect of benefitting two dice instead of just one die seems to be quite strong."
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html#probability-of-each-outcome",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html#probability-of-each-outcome",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "Probability of each outcome",
    "text": "Probability of each outcome\nFirst, we’ll look at distribution curves for both weapons under each of the conditions we just walked through. These curves will show the damage value rolled on the x-axis and the probability of rolling exactly that value on the \\(y\\)-axis.\n\n\nShow the code\nresults_processed |&gt;\n  ggplot() +\n  aes(x = value, y = rel_freq, color = which_gwf, shape = which_gwf) +\n  geom_point(alpha = 0.75, stroke = 1, size = 3) +\n  geom_line(alpha = 0.75, linewidth = 1) +\n  scale_x_continuous(\n    name = \"Damage result\",\n    breaks = seq(1, 12, 1),\n    minor_breaks = NULL,\n    limits = c(0.5, 12.5)\n  ) +\n  scale_y_continuous(\n    name = \"P(X = x)\",\n    breaks = scales::breaks_pretty(),\n    labels = scales::label_percent()\n  ) +\n  scale_color_brewer(\n    name = \"Rule\",\n    palette = \"Dark2\"\n  ) +\n  scale_shape_manual(\n    name = \"Rule\",\n    values = 15:17\n  ) +\n  facet_wrap(vars(weapon)) +\n  theme_minimal(base_size = 18) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nBy looking at the plot, we can see some interesting observations. Of course, the distributions for the 2d6 weapon damage are in general more interesting than for 1d12. For 1d12, we can see that the 2024 outcome really boosts the probability of rolling a 3 but is otherwise exactly the same as having no GWF, while the 2014 outcome slightly boosts the probability of each number 3 or larger but drastically lowers the chance of getting a 1 or 2.\nFor the 2d6 outcome, we can see that both corrections have an obvious impact that increases the average amount of damage, but in weird ways that are (at least to me) not incredibly intuitive. For the 2014 GWF distribution, 9 is the most common outcome, but for 2024, 9 is the second most common outcome after 6.\nSo, it seems that using either ability is obviously better than using neither. But it’s hard to tell whether the 2014 or the 2024 ability is better. It probably depends on our personal loss aversion bias – if you really don’t like rolling low numbers and want to more consistently reach a minimum value, then the 2024 version is better for you. However on the other hand, we can see that the 2014 version, we are more likely to roll high values. So if getting a few 1’s and 2’s is worth it for the times you get 10’s, 11’s, and 12’s, the 2014 version is for you.\nWe can see this more clearly if we look at the results a bit differently."
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html#probability-of-at-least-some-outcome",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html#probability-of-at-least-some-outcome",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "Probability of ‘at least’ some outcome",
    "text": "Probability of ‘at least’ some outcome\nSo far we’ve considered the probability that we roll exactly a specific outcome. But when we’re trying to decide which of the two options we prefer, it can be more helpful to look at the cumulative probabilities. The cumulative probability \\(P(X \\leq x)\\) can be interpreted as “the probability that we roll at most some value \\(x\\)”.\nFor many people, and certainly for me, it is typically easier to understand \\(P(X \\geq x)\\), “the probability that we roll at least some value \\(x\\)”. So that lets us answer the question “is the probability that I roll a 10 or more higher for the 2014 or 2024 GWF ability?” and other similar questions.\nFirst I’ll calculate those at least probabilities.\n\n\nShow the code\nresults_cumulative &lt;-\n    results_processed |&gt;\n    dplyr::group_by(which_gwf, weapon) |&gt;\n    dplyr::mutate(\n        at_most = cumsum(rel_freq),\n        at_least = 1 - at_most + rel_freq\n    ) |&gt;\n    dplyr::ungroup()\n\n\nNow we can make a plot with the probability we roll a value of \\(x\\) or higher on the y-axis.\n\n\nShow the code\nresults_cumulative |&gt;\n  ggplot() +\n  aes(x = value, y = at_least, color = which_gwf, shape = which_gwf) +\n  geom_point(alpha = 0.75, stroke = 1, size = 3) +\n  geom_line(alpha = 0.75, linewidth = 1) +\n  scale_x_continuous(\n    name = \"Damage result\",\n    breaks = seq(1, 12, 1),\n    minor_breaks = NULL,\n    limits = c(0.5, 12.5)\n  ) +\n  scale_y_continuous(\n    name = \"P(X ≥ x)\",\n    breaks = scales::breaks_pretty(),\n    labels = scales::label_percent()\n  ) +\n  scale_color_brewer(\n    name = \"Rule\",\n    palette = \"Dark2\"\n  ) +\n  scale_shape_manual(\n    name = \"Rule\",\n    values = 15:17\n  ) +\n  facet_wrap(vars(weapon)) +\n  theme_minimal(base_size = 18) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nYes, I think this nicely shows my conclusion, although you might disagree with me, and that’s ok! For both weapons, we can see that the 2024 rules have a higher probability of getting at least some minimum value (3 for 1d12, 6 for 2d6), but we have a lower probability of rolling at least ANY VALUE above that threshold!"
  },
  {
    "objectID": "posts/2024-09-22-Great-Weapon-Fighting/index.html#details",
    "href": "posts/2024-09-22-Great-Weapon-Fighting/index.html#details",
    "title": "2014 vs. 2024 Great Weapon Fighting",
    "section": "Details",
    "text": "Details\n\n\nAnother common tactic I use for this types of problems, which I call the lazy way, is just to write a simulation that replicates the behavior of interest a million times and look at the empirical probabilities. This is often a great strategy, but for this example the analytical computation is simple enough that it’s not worth doing a simulation. However, the more dice you start rolling, the more RAM it takes to enumerate combinations and the more worthwhile it becomes to just do a simulation. You can see my example simulation code in the code links.\nA much simpler way to do this is to use the specialized web app AnyDice by Jasper Flick. Implementing an AnyDice program for this comparison takes only 3 lines of code, although I’ve modified this so that \\(n\\), the number of faces on the dice, and \\(k\\), the number of dice to roll, are variables that you can easily change all at once.\nInformation from the Player’s Handbook is not owned by me, and is included here under fair use for educational purposes (although it seems prudent to mention that all of the information included here is also licensed under the Open Game License Version 1.0a and is included in the Basic Rules). The CC-BY-NC-SA licensed under which my content is not distributed does not extend to information which is owned by Wizards of the Coast or Hasbro.\nThis document was last updated at 2024-09-23 00:02:37.230328. The complete R session information is reproduced below.\n\n\n\nShow the code\nsessioninfo::session_info()\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.1 (2024-06-14 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/New_York\n date     2024-09-23\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cli            3.6.3   2024-06-21 [1] CRAN (R 4.4.1)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.4.1)\n digest         0.6.36  2024-06-23 [1] CRAN (R 4.4.1)\n dplyr          1.1.4   2023-11-17 [1] CRAN (R 4.4.1)\n evaluate       0.24.0  2024-06-10 [1] CRAN (R 4.4.1)\n fansi          1.0.6   2023-12-08 [1] CRAN (R 4.4.1)\n farver         2.1.2   2024-05-13 [1] CRAN (R 4.4.1)\n fastmap        1.2.0   2024-05-15 [1] CRAN (R 4.4.1)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.4.1)\n ggplot2      * 3.5.1   2024-04-23 [1] CRAN (R 4.4.1)\n glue           1.7.0   2024-01-09 [1] CRAN (R 4.4.1)\n gtable         0.3.5   2024-04-22 [1] CRAN (R 4.4.1)\n here           1.0.1   2020-12-13 [1] CRAN (R 4.4.1)\n htmltools      0.5.8.1 2024-04-04 [1] CRAN (R 4.4.1)\n jsonlite       1.8.8   2023-12-04 [1] CRAN (R 4.4.1)\n knitr          1.48    2024-07-07 [1] RSPM\n lifecycle      1.0.4   2023-11-07 [1] CRAN (R 4.4.1)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.4.1)\n munsell        0.5.1   2024-04-01 [1] CRAN (R 4.4.1)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.4.1)\n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.4.1)\n R6             2.5.1   2021-08-19 [1] CRAN (R 4.4.1)\n RColorBrewer   1.1-3   2022-04-03 [1] CRAN (R 4.4.0)\n renv           1.0.7   2024-04-11 [1] CRAN (R 4.4.1)\n rlang          1.1.4   2024-06-04 [1] CRAN (R 4.4.1)\n rmarkdown      2.27    2024-05-17 [1] CRAN (R 4.4.1)\n rprojroot      2.0.4   2023-11-05 [1] CRAN (R 4.4.1)\n rstudioapi     0.16.0  2024-03-24 [1] CRAN (R 4.4.1)\n scales         1.3.0   2023-11-28 [1] CRAN (R 4.4.1)\n sessioninfo    1.2.2   2021-12-06 [1] CRAN (R 4.4.1)\n tibble         3.2.1   2023-03-20 [1] CRAN (R 4.4.1)\n tidyselect     1.2.1   2024-03-11 [1] CRAN (R 4.4.1)\n utf8           1.2.4   2023-10-22 [1] CRAN (R 4.4.1)\n vctrs          0.6.5   2023-12-01 [1] CRAN (R 4.4.1)\n withr          3.0.0   2024-01-16 [1] CRAN (R 4.4.1)\n xfun           0.45    2024-06-16 [1] CRAN (R 4.4.1)\n yaml           2.3.9   2024-07-05 [1] RSPM\n\n [1] D:/proj/quarto-website/renv/library/windows/R-4.4/x86_64-w64-mingw32\n [2] C:/Users/Zane/AppData/Local/R/cache/R/renv/sandbox/windows/R-4.4/x86_64-w64-mingw32/e0da0d43\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-08-10_Titer-w-Distance/index.html",
    "href": "posts/2023-08-10_Titer-w-Distance/index.html",
    "title": "Simulating Titer Data with Distance as a Covariate",
    "section": "",
    "text": "In my previous blog post I talked about how to simulate a common type of immunological titer data, which is subject to both interval censoring and a lower limit of detection. In that blog post, we describing the data generating process for the observed values, and our simulation depended on the prespecification of the mean and variance of the distribution. In this blog post, we’ll discuss how we can extend the data generating process to include other variables. In particular, my research focuses on how antigenic distance can affect these titer values, but the way I’ll extend these simulations is very general.\nFirst let’s review the data-generating process – this is actually probably better written than my original post on this, because I’ve been thinking about these issues a lot and trying to improve my descriptions and notations."
  },
  {
    "objectID": "posts/2023-08-10_Titer-w-Distance/index.html#data-generating-process-without-covariates",
    "href": "posts/2023-08-10_Titer-w-Distance/index.html#data-generating-process-without-covariates",
    "title": "Simulating Titer Data with Distance as a Covariate",
    "section": "Data-generating process without covariates",
    "text": "Data-generating process without covariates\nIf we recall our DGP from the previous post, \\[\n\\begin{aligned}\nz_i^* &\\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right) \\\\\nz_i &= \\max\\left\\{0, \\left\\lfloor z_i^* \\right\\rfloor \\right\\} \\\\\ny_i &= g(z_i) = 5 \\cdot 2 ^{z_i}\n\\end{aligned}\n\\] This data generation process is composed of the following parts:\n\nThe log-scale latent titer, \\(z_i^*\\), which is a real-valued random variable;\nThe log-scale censored titer, \\(z_i\\), which is a nonnegative integer- valued random variable. This reflects the limit of detection – our transformation function \\(g(\\cdot)\\) which takes us from the natural scale to the log scale ensures that the limit of detection is represented as \\(0\\) on the log scale. So if \\(z_i\\) is below the log-scale LoD, it gets rounded up to \\(0\\). Otherwise, we take the floor of the underlying titer to represent interval-censoring in our measurements.\nThe natural-scale observed titer, \\(y_i\\), is a nonnegative integer on the natural scale and so takes values in the set \\(\\{5, 10, 20, 40, \\ldots \\}\\). This value has been censored and transformed back to the natural scale.\n\nThis data generating process assumes that the latent titers follow a log-normal distribution. Another plausible model would be a gamma distribution, which is somewhat more annoying to deal with, so we’ll deal with that in a future post.\nNow, I promised we would incorporate some covariates, so that’s what we’ll do next."
  },
  {
    "objectID": "posts/2023-08-10_Titer-w-Distance/index.html#perfectly-known-covariates-and-linear-models",
    "href": "posts/2023-08-10_Titer-w-Distance/index.html#perfectly-known-covariates-and-linear-models",
    "title": "Simulating Titer Data with Distance as a Covariate",
    "section": "Perfectly known covariates and linear models",
    "text": "Perfectly known covariates and linear models\nThere are a lot of different ways to incorporate covariates into this type of model, but the most common way by far is to make the mean titer value dependent on the value of some covariate, i.e., we would add the line \\[\\mu_i = f(x_i) \\] to the DGP above. Choosing the correct function here is a very complex problem which relies on “good-enough” approximations in the real world. One incredibly simple, but also very useful, function is the simple linear regression model. Linear models cover a wide range of patterns, but in the simple linear model, we assume that \\(f(x) = \\beta_0 + \\beta_1 x\\), which is a linear function of \\(x\\). For my specific example of incorporating antigenic distance, \\(d\\) into the data generating process, we’ll use this simple linear regression model.\nAt the risk of repeating a lot of text, our data generating process when the mean of the titer depends on the antigenic distance in a linear relationship is as follows.\n\\[\n\\begin{aligned}\nz_i^* &\\sim \\mathcal{N}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot d_i \\\\\nz_i &= \\max\\left\\{0, \\left\\lfloor z_i^* \\right\\rfloor \\right\\} \\\\\ny_i &= g(z_i) = 5 \\cdot 2 ^{z_i}\n\\end{aligned}\n\\]\nWe would expect \\(\\beta_0 &gt; 0\\) and \\(\\beta_1 &lt; 0\\) here. The parameter \\(\\beta_0\\) would correspond to the homologous immune response – we’re testing the HAI titer against the exact same strain a person has been exposed to. The parameter \\(\\beta_1\\) then represents the expected decrease in the immune response as a test strain becomes further away from the exposure strain. If we normalize our distances to fall in \\([0, 1]\\), where \\(0\\) is the homologous strain and \\(1\\) is the maximally different strain (although still not completely different, depending on the sample), we would also note that \\[E[y \\mid x = 1] - E[y \\mid x = 0] = \\mu(1) - \\mu(0) = \\beta_1,\\] so we can interpret \\(\\beta_1\\) as the difference in the mean response between the maximally distant strain and the homologous strain. If the maximally distant strain should represent a completely novel strain for which there should be no immune response, we could enforce the constraint \\[\\mu_i(1) = 0 \\implies \\beta_0 + \\beta_1 = 0,\\] which is linear and could be incorporated into a solution using a constrained maximum likelihood estimation framework.\nThe data is easy to simulate, similar to the previous example. For a test example, we assume that we can choose which antigenic distances to measure, and at each antigenic distance, we obtain some number of independent titer values (we could relax the independence assumption here by using a more complex model, but we choose not to deal with that right now).\nFor this example, we’ll set our distances for each individual to be \\(d_i = \\{0, 0.1, 0.2, \\ldots, 1.0\\}\\), and we’ll assume we have \\(1000\\) individuals. We collect their titers at each distance. For our simulation parameters, we’ll set \\(\\beta_0 = 4\\), \\(\\beta_1 = -3\\), and \\(\\sigma^2 = 2\\).\nWe start with the same simulation function we used previously.\n\none_titer_sim &lt;- function(N = 1e4, seed = 370, mean = 3, sd = 1) {\n    set.seed(seed)\n    sim &lt;-\n        tibble::tibble(\n            # Assume log(titer) is drawn from a normal distribution\n            raw_log_titer = rnorm(N, mean, sd),\n            # If we observe a titer with log(titer) &lt; 1 (LOD), mark it as 0\n            trunc_log_titer = ifelse(raw_log_titer &gt;= 1, raw_log_titer, 0),\n            # The assay is dilution based, so we only observe the floor of each\n            # value.\n            rounded_titer = floor(trunc_log_titer),\n            # Now final observed titer is equal to this transformation.\n            sim_titer = 5 * 2 ^ rounded_titer\n        ) |&gt;\n        dplyr::arrange(raw_log_titer)\n}\n\nThen, we calculate the value of \\(\\mu_i\\) at each value of \\(d_i\\), and draw 1000 observations at each \\(\\mu\\) value.\n\nex_dist_sim &lt;-\n    tibble::tibble(\n        d = seq(0, 1, 0.1),\n        mu = 4 - 3 * d\n    ) |&gt;\n    dplyr::mutate(\n        sim = purrr::map(mu, \\(x) one_titer_sim(1000, mean = x, sd = 2))\n    ) |&gt;\n    tidyr::unnest(sim)\n\nplt &lt;-\n    ex_dist_sim |&gt;\n    dplyr::mutate(\n        sim_titer = factor(sim_titer),\n        mu = factor(mu) |&gt; forcats::fct_inorder(),\n        distance = factor(d) |&gt; forcats::fct_inorder()\n    ) |&gt;\n    ggplot() +\n    aes(x = sim_titer) +\n    geom_bar(col = \"black\", fill = \"gray\") +\n    facet_wrap(~distance, labeller = \"label_both\") +\n    labs(\n        x = \"Observed titer\",\n        y = \"Count\"\n    ) +\n    theme(axis.text.x = element_text(angle = 45)) +\n    coord_cartesian(ylim = c(0, 700))\nfn &lt;- here::here(\"posts\", \"2023-08-10_Titer-w-Distance\", \"thumbnail.png\")\nggsave(\n    filename = fn,\n    plot = plt,\n    width = 13,\n    height = 8\n)\nknitr::include_graphics(fn)\n\n\n\n\n\n\n\n\nIf we simulate a more extreme example, say where \\(|\\beta_1| &gt; \\beta_0\\), we can generate titers which decay rapidly towards the limit of detection. In the next simulation, we left all simulation parameters the same except we set \\(\\beta_1 = -6\\).\n\nex_dist_sim &lt;-\n    tibble::tibble(\n        d = seq(0, 1, 0.1),\n        mu = 4 - 6 * d\n    ) |&gt;\n    dplyr::mutate(\n        sim = purrr::map(mu, \\(x) one_titer_sim(1000, mean = x, sd = 2))\n    ) |&gt;\n    tidyr::unnest(sim)\n\nplt &lt;-\n    ex_dist_sim |&gt;\n    dplyr::mutate(\n        sim_titer = factor(sim_titer),\n        mu = factor(mu) |&gt; forcats::fct_inorder(),\n        distance = factor(d) |&gt; forcats::fct_inorder()\n    ) |&gt;\n    ggplot() +\n    aes(x = sim_titer) +\n    geom_bar(col = \"black\", fill = \"gray\") +\n    facet_wrap(~distance, labeller = \"label_both\") +\n    labs(\n        x = \"Observed titer\",\n        y = \"Count\"\n    ) +\n    theme(axis.text.x = element_text(angle = 45)) +\n    coord_cartesian(ylim = c(0, 700))\nfn &lt;- here::here(\"posts\", \"2023-08-10_Titer-w-Distance\", \"p2.png\")\nggsave(\n    filename = fn,\n    plot = plt,\n    width = 13,\n    height = 8\n)\nknitr::include_graphics(fn)"
  },
  {
    "objectID": "posts/2023-08-10_Titer-w-Distance/index.html#conclusions",
    "href": "posts/2023-08-10_Titer-w-Distance/index.html#conclusions",
    "title": "Simulating Titer Data with Distance as a Covariate",
    "section": "Conclusions",
    "text": "Conclusions\nIn this relatively short post, I showed a quick example of including distance as a covariate in a linear model for titers. There are a couple more things we need to do for an HAI manifesto though, and we’re currently working on them right now! Not sure if they will be blog posts or part of something else though. Importantly, we might be interested in\n\nModeling the post-vaccination titer as a function of distance and pre-vaccination titer;\nCovariates with their own parametric models, such as for measurement error;\nOn the idea of measurement error, accounting for measurement error in the outcome variable along with censoring; and most importantly\nHow do we account for all of those issues in a statistical model, and hopefully recover the parameters of the known DGP?"
  },
  {
    "objectID": "posts/2023-08-10_Titer-w-Distance/index.html#details",
    "href": "posts/2023-08-10_Titer-w-Distance/index.html#details",
    "title": "Simulating Titer Data with Distance as a Covariate",
    "section": "Details",
    "text": "Details\n\nLast updated at 2023-11-06 10:54:38.522226.\nsource code\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.4\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.1      jsonlite_1.8.7    dplyr_1.0.10      compiler_4.3.1   \n [5] renv_1.0.3        tidyselect_1.2.0  stringr_1.4.1     tidyr_1.2.1      \n [9] scales_1.2.1      yaml_2.3.6        fastmap_1.1.0     here_1.0.1       \n[13] R6_2.5.1          labeling_0.4.2    generics_0.1.3    knitr_1.40       \n[17] forcats_1.0.0     htmlwidgets_1.5.4 tibble_3.1.8      rprojroot_2.0.3  \n[21] munsell_0.5.0     pillar_1.8.1      rlang_1.1.1       utf8_1.2.2       \n[25] stringi_1.7.8     xfun_0.34         cli_3.6.1         withr_2.5.0      \n[29] magrittr_2.0.3    digest_0.6.33     grid_4.3.1        lifecycle_1.0.3  \n[33] vctrs_0.5.0       hgp_0.0.1         evaluate_0.23     glue_1.6.2       \n[37] farver_2.1.1      fansi_1.0.3       colorspace_2.0-3  rmarkdown_2.17   \n[41] purrr_0.3.5       tools_4.3.1       pkgconfig_2.0.3   ellipsis_0.3.2   \n[45] htmltools_0.5.3"
  },
  {
    "objectID": "posts/2023-07-25_Simulating-Titer-Data/index.html",
    "href": "posts/2023-07-25_Simulating-Titer-Data/index.html",
    "title": "Simulating Titer Data",
    "section": "",
    "text": "A sample histogram showing what exciting results are to come!\nSo this will probably be a shorter post, but it will hopefully be the first in a series of posts about some immunological data problems I’m working on right now. In this post, I’ll give a brief background on immunological titer data, a short explanation of why I care, and then I’ll walk through the generative model I’ve adopted for simulating titer data. I plan to explain the steps of my model and show how each part works both in math and in R code. Of course I will have to make some simplifying assumptions, but I’ll try to explain what those are and what any alternatives might be."
  },
  {
    "objectID": "posts/2023-07-25_Simulating-Titer-Data/index.html#details",
    "href": "posts/2023-07-25_Simulating-Titer-Data/index.html#details",
    "title": "Simulating Titer Data",
    "section": "Details",
    "text": "Details\n\nLast updated at 2023-07-25 22:28:08.82951.\nsource code\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.5.0       cli_3.4.1         knitr_1.40        rlang_1.0.6      \n [5] xfun_0.34         stringi_1.7.8     renv_0.16.0       generics_0.1.3   \n [9] jsonlite_1.8.3    glue_1.6.2        htmltools_0.5.3   fansi_1.0.3      \n[13] rmarkdown_2.17    evaluate_0.17     tibble_3.1.8      fastmap_1.1.0    \n[17] yaml_2.3.6        lifecycle_1.0.3   stringr_1.4.1     compiler_4.3.0   \n[21] dplyr_1.0.10      htmlwidgets_1.5.4 pkgconfig_2.0.3   digest_0.6.30    \n[25] R6_2.5.1          tidyselect_1.2.0  utf8_1.2.2        pillar_1.8.1     \n[29] magrittr_2.0.3    tools_4.3.0"
  },
  {
    "objectID": "presentations/Open-Science-IDIG/index.html",
    "href": "presentations/Open-Science-IDIG/index.html",
    "title": "Intro to Open Science",
    "section": "",
    "text": "In January 2023, I was awarded a fellowship by the Center for Open Science, funded by FluLab. I got some excellent open science training, and one stipulation of my fellowship was that I had to dissiminate my training knowledge with my colleagues. So I talked about what open science is, why bother with open science, how to start doing open science, and briefly about the Open Science Foundation, an online platform developed by OSF to empower open science workflows. The slides are embedded below or you can get them here if you’re interested."
  },
  {
    "objectID": "presentations/IDIG-Challenge-Studies/index.html",
    "href": "presentations/IDIG-Challenge-Studies/index.html",
    "title": "Challenge Study Ethics",
    "section": "",
    "text": "My colleague Savannah and I led a discussion on challenge study ethics at the seminar I coordinate (IDIG, the Infectious Disease Interest Group). The main topic was how to handle including unethical studies in systematic reviews, but we also discussed several other issues related to challenge studies. We only got through half of the powerpoint because we both love to talk, but the slides are embedded below or you can get them here if you’re interested."
  },
  {
    "objectID": "presentations/Dissertation-Proposal/index.html",
    "href": "presentations/Dissertation-Proposal/index.html",
    "title": "Dissertation Proposal",
    "section": "",
    "text": "This is my dissertation proposal. Here’s the abstract:\nInfluenza is a respiratory disease that occurs in seasonal epidemics worldwide. The Centers for Disease Control and Prevention recommend annual vaccination against seasonal influenza with a modified vaccine for adults in the United States, but the effectiveness of these vaccines varies greatly across seasons and individuals. Improved understanding of the drivers of the immune response following vaccination is crucial for improving influenza vaccines.\nThe slides are embedded below or you can get them here if you’re interested. If you want, you can read the full proposal here."
  },
  {
    "objectID": "presentations/CEIRR-CMC-2024/index.html",
    "href": "presentations/CEIRR-CMC-2024/index.html",
    "title": "Modeling Censored Immunological Data",
    "section": "",
    "text": "I presented these slides at the CEIRR Annual Meeting 2024, specifically at the Computational Modeling Core tools showcase. They introduce and provide some motivation for a set of tutorials on censored data that we are currently developing (at time of writing). The slides are embedded below or you can get them here if you’re interested.\nYou can access our censored data tutorials at this link: https://tinyurl.com/hg-cens."
  },
  {
    "objectID": "presentations/2022-CIVIC/index.html",
    "href": "presentations/2022-CIVIC/index.html",
    "title": "Longitudinal trajectories of influenza immune response after repeat vaccination",
    "section": "",
    "text": "I gave a two-minute lightning talk at the CIVR-HRP (Center for Influenza) annual meeting in Spring 2022. CIVR-HRP is an NIH-funded CIVIC (Collaborative Influenza Vaccine Innovation Centers) site at the University of Georgia, Athens, GA, USA. There were no poster presentations, so I got to give an actual talk.\nInstead of focusing on math and formal statistical analyses, my goal was to describe a conceptual framework (that I believe is novel) for analyzing longitudinal data. Often analyzing an individual’s entire trajectory over several years can be difficult to interpret, but by breaking trajectories down into 3 data points (day 0, day 21/28, and day 365 relative to the yearly vaccination), we can easily examine patterns of boosting and waning.\nIn the future, we intend to use this framework to quantify differences across strata, such as age groups. We also plan to include formal statistical analyses which can account for within-individual correlations.\nThe slides can be found here. They are also embedded below."
  },
  {
    "objectID": "presentations/2021-CIVR-HRP/index.html",
    "href": "presentations/2021-CIVR-HRP/index.html",
    "title": "How does pre-existing immunity interact with other factors to impact influenza vaccine responses?",
    "section": "",
    "text": "I gave a two-minute lightning talk at the CIVR-HRP (Center for Influenza) annual meeting in Fall 2021. CIVR-HRP is an NIH-funded CIVIC (Collaborative Influenza Vaccine Innovation Centers) site at the University of Georgia, Athens, GA, USA, but the meeting was held virtually due to COVID-19. There were no virtual poster presentations, so I gave a lightning talk instead.\nThe slides can be found here. They are also embedded below. In short, we used vaccine cohort data, and found that the data were consistent with some predictions of prior mechanistic models, but not all."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nQuantifying the effect of HD vs SD FluZone vaccination on heterologous immunity\n\n\n\n\n\n\ntalk\n\n\nresearch\n\n\n\nResearch talk given at the 2024 DIVERsity (drivers of individual variation in influenza vaccine response and protection from infection) grant annual meeting. We use data from a human influenza vaccine cohort study with HAIs collected to a panel of historical strains to analyze the causal effect of high dose vaccination on the heterologous antibody response. \n\n\n\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Censored Immunological Data\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nPublicity for the censored data tutorials we’re writing! \n\n\n\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying the breadth of vaccine response with antigenic distance\n\n\n\n\n\n\ntalk\n\n\nflu\n\n\nvaccines\n\n\nresearch\n\n\n\nWhat does it even mean when we talk about vaccine breadth? If we have a universal vaccine candidate, how can we calculate its breadth? We propose a framework utilizing antigenic distance and summary antibody landscapes based on data from cohort studies with panels of immunogenicity data to multiple strains.\n\n\n\n\n\nAug 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible research workshop\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nIntro to reproducible research and R workflows for PopBio\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDissertation Proposal\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nImpact of Host and Vaccine Characteristics on Immune Responses following Influenza Vaccination\n\n\n\n\n\nApr 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Open Science, v2\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nOpen science is a pretty broad term that broadly includes data sharing, open access publication, inclusive research, preregistration, and more! In this talk, I gave an intro to the concept of open science at a seminar for the Center for the Ecology of Infectious Disease (CEID) at UGA.\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Open Science\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nOpen science is a pretty broad term that broadly includes data sharing, open access publication, inclusive research, preregistration, and more! In this talk, I gave an intro to the concept of open science for my weekly working group.\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nJournal club (Nauta et al.)\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nSlides from a paper discussion that I led at my seminar.\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge Study Ethics\n\n\n\n\n\n\ntalk\n\n\nother\n\n\n\nChallenge studies need to be highly regulated to ensure they are beneficial rather than harmful. I led a discussion with one of my colleagues about some of these ethical issues.\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal trajectories of influenza immune response after repeat vaccination\n\n\n\n\n\n\ntalk\n\n\nflu\n\n\nvaccines\n\n\nresearch\n\n\ntrajectories\n\n\n\nRepeat vaccination is sort of a hot topic in flu immunity right now, but most analyses are still done in a cross-sectional format. Using data from an ongoing influenza vaccination cohort study, we break trajectories up into component segments to analyze patterns of boosting and waning. This allows us to quantify trends across strata and allow for measurement error in responses of a specific magnitude. \n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow do pre-existing immunity and host factors interact to impact influenza vaccine response?\n\n\n\n\n\n\ntalk\n\n\nflu\n\n\nvaccines\n\n\nresearch\n\n\n\nSome prior mechanistic modeling studies show that dose should modulate the effect of an influenza vaccine, but do not make strong predictions about the roles of covariates. Using data from an ongoing cohort study, we examine these predictions and also examine whether other covariates should be included in future models.\n\n\n\n\n\nAug 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the effect of host factors on the relationship between pre-existing immunity and influenza vaccine response\n\n\n\n\n\n\nposter\n\n\nflu\n\n\nvaccines\n\n\nresearch\n\n\n\nMechanistic models suggest that fold change in flu immunity after vaccination should be linearly decreasing with pre-vaccination immunity. However, these models do not account for host factors, such as age. We found that mechanistic model predictions were sometimes supported by the data, and we are working to explore when they are and are not.\n\n\n\n\n\nJun 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow does pre-existing immunity interact with other factors to impact influenza vaccine responses?\n\n\n\n\n\n\ntalk\n\n\nflu\n\n\nvaccines\n\n\nresearch\n\n\n\nPrevious mechanistic models predict that fold change in influenza antibody level should have a negative linear relationship with pre-vaccination titer on a log-log scale. Models also predict that higher vaccine doses should have a higher intercept, but parallel slopes. Using vaccine cohort data, we found that the first conclusion was true, but the latter was not always true. Now we are working to understand in which cases the model deviates from expected predictions. \n\n\n\n\n\nApr 6, 2021\n\n\nW. Zane Billings\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Zlog (Zane’s Blog)",
    "section": "",
    "text": "This is where I plan to post about different things that I’m interested in. Andrew Heiss gave me the excellent advice that all my work should be done in public because you never know what will be useful to someone else, and some of my adviser Andreas Handel’s posts have been very helpful to me. So maybe one day my blog will be as useful as their blogs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014 vs. 2024 Great Weapon Fighting\n\n\n\n\n\n\ndnd\n\n\ndice problems\n\n\n\nThe new D&D Player’s Handbook (5th edition 2024) came out recently, and the Great Weapon Fighting fighting style is a bit different. But is it better, or is the old one better? \n\n\n\n\n\nSep 22, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions for an Easy Quarto Website\n\n\n\n\n\nJust a list of instructions for building a quarto website with netlify, no GitHub, no CSS, nothing too technical. \n\n\n\n\n\nJul 13, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nIncrease vs post titer as regression outcome\n\n\n\n\n\nLots of people debate about whether titer increase (log of fold change) or raw post-vaccination titer should be used as the outcome for a regression model of immunological data. Under certain generative models however, these models are essentially equivalent and we can show how. \n\n\n\n\n\nOct 28, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Bootstrap CIs\n\n\n\n\n\nOne of my lab mates wanted to calculate bootstrap CIs for a project, which is something I know how to do. So I wanted to write a basic tutorial on bootstrap confidence intervals. \n\n\n\n\n\nOct 9, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Titer Data with Distance as a Covariate\n\n\n\n\n\nIn the second part of the HAI data manifesto, we’ll discuss how to incorporate a single covariate into the data generating process. \n\n\n\n\n\nAug 10, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Titer Data\n\n\n\n\n\nSo I work with a lot of immunological titer data, particularly HAI titers for flu. And I’ve recently needed to work out how to do some simulations. So this post will cover the generative model I’ve developed and show some examples, while hopefully explaining each step in a way. Mostly so my adviser can tell me what he thinks I did wrong. \n\n\n\n\n\nJul 25, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram binwidths\n\n\n\n\n\nThe shortest possible description of histogram binwidths in R \n\n\n\n\n\nDec 17, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple diagnostic testing and Bayes’ rule\n\n\n\n\n\nWhen you do multiple diagnostic tests in sequence, how do you estimate your probability of the condition? \n\n\n\n\n\nJun 24, 2022\n\n\nZane Billings\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "uses.html",
    "href": "uses.html",
    "title": "Uses",
    "section": "",
    "text": "This is a list of everything I use that I could think of to list here. If I forgot anything, feel free to let me know.\n\n\nI actually have three hardware setups. It’s kind of annoying and a big reason that I use Git for everything (but more on that later).\n\nIn my office I use a Dell-prebuilt Precision 5820 with Windows 10 Enterprise. The processor is an Intel Core i9-10980XE 3.00 GHz; 64Gb RAM; AMD Radeon Pro WX 2100 processor.\nIn my house I use a Dell prebuilt that I’ll have to fill in the details for eventually cause I never remember.\nOn the go I use a 2017 12” macbook with the retina screen. It’s super lightweight and I liked that a lot while carrying it everywhere in undergrad, but now I find it very underwhelming for the price point and probably will never buy another Mac.\nMy phone is a Samsung Galaxy A50. I leave it on do-not-disturb so if you aren’t my parents or sister it is unlikely that I will see your texts or calls in a reasonable amount of time.\n\n\n\n\n\nI almost exclusively use R these days.\nSometimes I use Stan. Most of what I do can be facilitated with the R packages rethinking or brms but I’ve been moving more towards using my own Stan code recently.\nMy preferred IDE for everything is RStudio. I use the “Vibrant Ink” theme that comes packaged with the IDE.\nIn the past I’ve used Python (with atom) and MATLAB/Octave (with the GUI), but not so much anymore.\nOn Windows, I exclusively use git bash as my terminal. On MacOS, I use the default terminal but set the language to bash.\nI use Anonymous Pro as my preferred coding font.\n\n\n\n\n\nI use Git and GitHub for nearly everything. Almost all of my new projects (even small things) start by creating an R project with a git repo.\nSpeaking of R projects, I use a .rproj file to organize anything I will do that will involve any amount of R code.\nFor almost all of my research-related writing, I use R Markdown, which is an implementation of pandoc-flavored markdown that can integrate code and the results from running code.\nI’ve recently started using Quarto\nFor course projects that require me to use SAS or don’t involve coding, I use LaTeX. I use MiKTeX for my distribution and TeXStudio for my editor. Although I’m considering just using TinyTex and overleaf since I rarely use LaTeX standalone anymore.\nI use Zotero for almost all of my reference management, although I’ve unfortunately been forced to use EndNote (which I will not provide a link to as I do not endorse it or Clarivate) recently.\nI use bibtex for all of my bibliographies as it works seamlessly with both pandoc and LaTeX.\nI use WinCompose for typing most special characters on Windows.\nI use google drive for anything non-code-related that I need to share. One day I’ll consider switching to Quarto when collaborative editing is out, but that isn’t today. If anyone knows a good markdown alternative, let me know.\nI’ve mostly switched to Quarto for all of my presentations. I still use Powerpoint occasionally (which I hate), but that’s only because I dislike CSS fiddling more than manual fiddling. But Quarto presentations are so much easier than either the old R Markdown presentations or beamer, and they look way better.\n\n\n\n\n\nIn the past I’ve used Mathematica, but apparently Julia can do everything that Mathematica can do, so I don’t recommend that anyone pay money for Mathematica.\nI’m also pretty good at SAS, but I will never buy a SAS license. Whenever I am forced to use SAS, I use SAS OnDemand for Academics. However, if you are a SAS user, I recommend pivoting away from SAS.\nI do not like using Microsoft Office. But I often find myself using Outlook and OneNote because figuring out anything else would take me time that I do not currently have.\nI use Firefox as my main browser, and I use google chrome when print to PDF in Firefox doesn’t work. Chrome is also required for some R/pandoc procedures for turning HTML into PDF.\nThis website is created with quarto, with source code on GitHub, and deployed by Netlify.\nI manage all my passwords with BitWarden.\nMy time and project management skills are very bad. I use a system cobbled together from OneNote, Outlook’s calendar, sticky notes in the office, and random scribblings on a notepad to manage what I need to get done. Usually it does not work very well, but I’ve never found a system that works better for me."
  },
  {
    "objectID": "uses.html#hardware",
    "href": "uses.html#hardware",
    "title": "Uses",
    "section": "",
    "text": "I actually have three hardware setups. It’s kind of annoying and a big reason that I use Git for everything (but more on that later).\n\nIn my office I use a Dell-prebuilt Precision 5820 with Windows 10 Enterprise. The processor is an Intel Core i9-10980XE 3.00 GHz; 64Gb RAM; AMD Radeon Pro WX 2100 processor.\nIn my house I use a Dell prebuilt that I’ll have to fill in the details for eventually cause I never remember.\nOn the go I use a 2017 12” macbook with the retina screen. It’s super lightweight and I liked that a lot while carrying it everywhere in undergrad, but now I find it very underwhelming for the price point and probably will never buy another Mac.\nMy phone is a Samsung Galaxy A50. I leave it on do-not-disturb so if you aren’t my parents or sister it is unlikely that I will see your texts or calls in a reasonable amount of time."
  },
  {
    "objectID": "uses.html#coding",
    "href": "uses.html#coding",
    "title": "Uses",
    "section": "",
    "text": "I almost exclusively use R these days.\nSometimes I use Stan. Most of what I do can be facilitated with the R packages rethinking or brms but I’ve been moving more towards using my own Stan code recently.\nMy preferred IDE for everything is RStudio. I use the “Vibrant Ink” theme that comes packaged with the IDE.\nIn the past I’ve used Python (with atom) and MATLAB/Octave (with the GUI), but not so much anymore.\nOn Windows, I exclusively use git bash as my terminal. On MacOS, I use the default terminal but set the language to bash.\nI use Anonymous Pro as my preferred coding font."
  },
  {
    "objectID": "uses.html#workflow-writing",
    "href": "uses.html#workflow-writing",
    "title": "Uses",
    "section": "",
    "text": "I use Git and GitHub for nearly everything. Almost all of my new projects (even small things) start by creating an R project with a git repo.\nSpeaking of R projects, I use a .rproj file to organize anything I will do that will involve any amount of R code.\nFor almost all of my research-related writing, I use R Markdown, which is an implementation of pandoc-flavored markdown that can integrate code and the results from running code.\nI’ve recently started using Quarto\nFor course projects that require me to use SAS or don’t involve coding, I use LaTeX. I use MiKTeX for my distribution and TeXStudio for my editor. Although I’m considering just using TinyTex and overleaf since I rarely use LaTeX standalone anymore.\nI use Zotero for almost all of my reference management, although I’ve unfortunately been forced to use EndNote (which I will not provide a link to as I do not endorse it or Clarivate) recently.\nI use bibtex for all of my bibliographies as it works seamlessly with both pandoc and LaTeX.\nI use WinCompose for typing most special characters on Windows.\nI use google drive for anything non-code-related that I need to share. One day I’ll consider switching to Quarto when collaborative editing is out, but that isn’t today. If anyone knows a good markdown alternative, let me know.\nI’ve mostly switched to Quarto for all of my presentations. I still use Powerpoint occasionally (which I hate), but that’s only because I dislike CSS fiddling more than manual fiddling. But Quarto presentations are so much easier than either the old R Markdown presentations or beamer, and they look way better."
  },
  {
    "objectID": "uses.html#other-software",
    "href": "uses.html#other-software",
    "title": "Uses",
    "section": "",
    "text": "In the past I’ve used Mathematica, but apparently Julia can do everything that Mathematica can do, so I don’t recommend that anyone pay money for Mathematica.\nI’m also pretty good at SAS, but I will never buy a SAS license. Whenever I am forced to use SAS, I use SAS OnDemand for Academics. However, if you are a SAS user, I recommend pivoting away from SAS.\nI do not like using Microsoft Office. But I often find myself using Outlook and OneNote because figuring out anything else would take me time that I do not currently have.\nI use Firefox as my main browser, and I use google chrome when print to PDF in Firefox doesn’t work. Chrome is also required for some R/pandoc procedures for turning HTML into PDF.\nThis website is created with quarto, with source code on GitHub, and deployed by Netlify.\nI manage all my passwords with BitWarden.\nMy time and project management skills are very bad. I use a system cobbled together from OneNote, Outlook’s calendar, sticky notes in the office, and random scribblings on a notepad to manage what I need to get done. Usually it does not work very well, but I’ve never found a system that works better for me."
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Other stuff",
    "section": "",
    "text": "ASCII (.txt) version of my CV: here.\nMy OEIS (On-line Encyclopedia of Integer Sequences) contributions: here. (I made a few contributions during undergrad and haven’t though about integer sequences since then.)\n\n\n\n\n\nCourse teaching\n\nTeaching Assistant, Fall 2022, EPID 7500: Introduction to Coding in R, Data Science and Simulation for Public Health and the Life Sciences; University of Georgia.\nLab assistant, Spring 2017 - Spring 2019, MATH 340: Scientific Computing; Western Carolina University.\n\nProgramming workshops\n\nReproducible research with R / R Markdown, June 2022. Led a workshop for 12 undergraduate participants in the Population Biology of Infectious Diseases REU Program (PopBio).\nR/SAS primer, August 2021. Organized and led a workshop introducing basic R and SAS skills for College of Public Health students (MS, MPH, and PhD).\nData visualization using ggplot2, June 2021. Hosted a workshop for 10 undergrads in the PopBio program.\n\nUndergraduate mentoring\n\nPrimary mentor for one undergraduate student for the PopBio 2022 program, leading to an in-progress manuscript.\nCo-mentor for one undergraduate student for the PopBio 2021 program, leading to an in-progress manuscript.\n\nPeer mentoring\n\nSupplementary Instruction (SI) leader, Spring 2020, Calculus II; Western Carolina University.\nCourse tutor, Spring 2017 - Spring 2019; Western Carolina University. Individual and group tutoring for intro biology, organic chemsitry, genetics, evolutionary biology, microscopy, and immunology.\n\n\n\n\n\n\nCurriculum committee student representative, Department of Epidemiology & Biostatistics, UGA. Fall 2021 to present.\nPresident, Graduate Scholars of Epidemiology and Biostatistics, UGA. Fall 2021 - Spring 2022."
  },
  {
    "objectID": "misc.html#links",
    "href": "misc.html#links",
    "title": "Other stuff",
    "section": "",
    "text": "ASCII (.txt) version of my CV: here.\nMy OEIS (On-line Encyclopedia of Integer Sequences) contributions: here. (I made a few contributions during undergrad and haven’t though about integer sequences since then.)"
  },
  {
    "objectID": "misc.html#teaching-experience",
    "href": "misc.html#teaching-experience",
    "title": "Other stuff",
    "section": "",
    "text": "Course teaching\n\nTeaching Assistant, Fall 2022, EPID 7500: Introduction to Coding in R, Data Science and Simulation for Public Health and the Life Sciences; University of Georgia.\nLab assistant, Spring 2017 - Spring 2019, MATH 340: Scientific Computing; Western Carolina University.\n\nProgramming workshops\n\nReproducible research with R / R Markdown, June 2022. Led a workshop for 12 undergraduate participants in the Population Biology of Infectious Diseases REU Program (PopBio).\nR/SAS primer, August 2021. Organized and led a workshop introducing basic R and SAS skills for College of Public Health students (MS, MPH, and PhD).\nData visualization using ggplot2, June 2021. Hosted a workshop for 10 undergrads in the PopBio program.\n\nUndergraduate mentoring\n\nPrimary mentor for one undergraduate student for the PopBio 2022 program, leading to an in-progress manuscript.\nCo-mentor for one undergraduate student for the PopBio 2021 program, leading to an in-progress manuscript.\n\nPeer mentoring\n\nSupplementary Instruction (SI) leader, Spring 2020, Calculus II; Western Carolina University.\nCourse tutor, Spring 2017 - Spring 2019; Western Carolina University. Individual and group tutoring for intro biology, organic chemsitry, genetics, evolutionary biology, microscopy, and immunology."
  },
  {
    "objectID": "misc.html#service",
    "href": "misc.html#service",
    "title": "Other stuff",
    "section": "",
    "text": "Curriculum committee student representative, Department of Epidemiology & Biostatistics, UGA. Fall 2021 to present.\nPresident, Graduate Scholars of Epidemiology and Biostatistics, UGA. Fall 2021 - Spring 2022."
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "What is Zane doing right now?",
    "section": "",
    "text": "As of 2023-03-06, this is what I am working on.\n\n\n\nI’ll be joining StateFarm as a data science intern over the summer! Wow!! So I’ll have a full time job and probably won’t get too much research done, but I’ll try my best.\nI’m formatting and getting ready to submit a paper about differences between clinical and patient reports of influenza symptoms. Huge shout-out to the great undergrad students I got to work with on this one: Annika Cleven and Jackie Dworaczyk.\nWriting my dissertation proposal, broadly on these areas.\n\nHow well did past vaccines confer immunity to future (at the time, they are past now) strains?\nIs antigenic distance between vaccine strains and circulating strains important for determining how protective the vaccine will be? Fortunately, I’ve worked with a great colleague who previously did a lot of hard work on this.\nWhat is the best way to quantify the “breadth” of a flu response? That is, if you get a vaccine, how can we measure the extent of which flu strains you’ll be protected against?\n\nI’m still reading Statistical Rethinking by Richard McElreath. I’m currently working on the Chapter 6 exercises. You can see my progress here.\nThe norovirus challenge review (finding every norovirus challenge study that’s ever been conducted) is currently on hold for a bit, but I hope to come back to it after my proposal and stuff.\n\n\n\n\n\nMy colleague Yang Ge is now a professor (yay Yang!) and has submitted the norovirus Bayesian modeling paper I did some work on.\nMaking a lot of figures for a longitudinal analysis of influenza immune response trajectories. Most of the paper and analysis is being done by Meng-Hsuan Sung.\n\n\n\n\n\nI’m the TA for my adviser’s Modern Applied Data Analysis course. We’re having a lot of fun doing machine learning and that kind of stuff.\nCurrently trying to read, when I get a chance:\n\nAnnihilation by Jeff VanderMeer\nThe Stand by Stephen King\nBayesian Statistics the Fun Way by Will Kurt"
  },
  {
    "objectID": "now.html#my-projects",
    "href": "now.html#my-projects",
    "title": "What is Zane doing right now?",
    "section": "",
    "text": "I’ll be joining StateFarm as a data science intern over the summer! Wow!! So I’ll have a full time job and probably won’t get too much research done, but I’ll try my best.\nI’m formatting and getting ready to submit a paper about differences between clinical and patient reports of influenza symptoms. Huge shout-out to the great undergrad students I got to work with on this one: Annika Cleven and Jackie Dworaczyk.\nWriting my dissertation proposal, broadly on these areas.\n\nHow well did past vaccines confer immunity to future (at the time, they are past now) strains?\nIs antigenic distance between vaccine strains and circulating strains important for determining how protective the vaccine will be? Fortunately, I’ve worked with a great colleague who previously did a lot of hard work on this.\nWhat is the best way to quantify the “breadth” of a flu response? That is, if you get a vaccine, how can we measure the extent of which flu strains you’ll be protected against?\n\nI’m still reading Statistical Rethinking by Richard McElreath. I’m currently working on the Chapter 6 exercises. You can see my progress here.\nThe norovirus challenge review (finding every norovirus challenge study that’s ever been conducted) is currently on hold for a bit, but I hope to come back to it after my proposal and stuff."
  },
  {
    "objectID": "now.html#collaborations",
    "href": "now.html#collaborations",
    "title": "What is Zane doing right now?",
    "section": "",
    "text": "My colleague Yang Ge is now a professor (yay Yang!) and has submitted the norovirus Bayesian modeling paper I did some work on.\nMaking a lot of figures for a longitudinal analysis of influenza immune response trajectories. Most of the paper and analysis is being done by Meng-Hsuan Sung."
  },
  {
    "objectID": "now.html#not-research-stuff",
    "href": "now.html#not-research-stuff",
    "title": "What is Zane doing right now?",
    "section": "",
    "text": "I’m the TA for my adviser’s Modern Applied Data Analysis course. We’re having a lot of fun doing machine learning and that kind of stuff.\nCurrently trying to read, when I get a chance:\n\nAnnihilation by Jeff VanderMeer\nThe Stand by Stephen King\nBayesian Statistics the Fun Way by Will Kurt"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "W. Zane Billings",
    "section": "",
    "text": "I too am not a bit tamed, I too am untranslatable, I sound my barbaric yawp over the roofs of the world.\n– Walt Whitman, Leaves of Grass\n\nI am currently a PhD student working with Andreas Handel at the University of Georgia (Department of Epidemiology & Biostatistics). My main skills are R programming, scientific computing, and data analysis. I like using data science and statistical methods to inform cool questions about viruses, mainly flu.\n\nEducation\nPhD, Epidemiology and biostatistics. August 2020 – Present. University of Georgia, Athens, GA, USA.\nBS, Mathematics and biology. August 2016 – May 2020. Western Carolina University, Cullowhee, NC, USA.\n\n\nInterests\n\n(Bayesian) hierarchical/multilevel models for epidemiological problems\nEffects of antigenic drift on immunity and vaccination\nEpidemiology and immunology of influenza"
  },
  {
    "objectID": "posts/2024-07-13-Quarto-Website-Checklist/index.html",
    "href": "posts/2024-07-13-Quarto-Website-Checklist/index.html",
    "title": "Instructions for an Easy Quarto Website",
    "section": "",
    "text": "This checklist is 100% adapted from a much better blog post by Jadey Ryan. This is a condensed version that is designed to print on ~1 (maybe front/back) letter sized page.\n\nSteps for making a Quarto website with Netlify.\n\nInstall R and RStudio, update Quarto.\n\nR download link: https://cran.r-project.org/.\nRStudio download link: https://posit.co/download/rstudio-desktop/.\nQuarto download link: https://quarto.org/docs/get-started/.\n\nOpen RStudio, and use the dialog menu to create a new project in a new directory. Choose the “Quarto Website” project type. Optionally, check the “use visual editor” option.\n(Optional) render the website for the first time.\nEdit the home page (index.qmd). Specify the title and homepage contents.\nEdit the about page (about.qmd), using a template from the quarto docs. Add an image if you have one.\nAdd a new page called “Projects” along with a directory, and set it up as a listing page.\nAdd a new test project to the listing.\nEdit the listing _metadata.yml so that code pages get frozen.\nUpdate _quarto.yml to show the listing page, also talk about updating the website theme. See themes here.\nMake sure everything renders.\nCreate an account on netlify if you don’t have one, then create a new netlify site. Upload the entire _site folder to Netlify.\nIn site configuration, set up the site name. You can also add a custom domain name here if you want to pay for one.\nWhenever you want to update your website, simply render it again, and upload the _site folder to your website.\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{billings2024,\n  author = {Billings, Zane},\n  title = {Instructions for an {Easy} {Quarto} {Website}},\n  date = {2024-07-13},\n  url = {https://wzbillings.com/posts/2024-07-13-Quarto-Website-Checklist/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBillings, Zane. 2024. “Instructions for an Easy Quarto\nWebsite.” July 13, 2024. https://wzbillings.com/posts/2024-07-13-Quarto-Website-Checklist/."
  },
  {
    "objectID": "presentations/2021-CIVIC/index.html",
    "href": "presentations/2021-CIVIC/index.html",
    "title": "How do pre-existing immunity and host factors interact to impact influenza vaccine response?",
    "section": "",
    "text": "I gave a two-minute “lighting presentation” at the NIAID CIVICs virtual meeting in 2021. You can find my slides here as well as embedded at the bottom of the page.\nIn brief, some prior mechanistic modeling studies show that dose should modulate the effect of an influenza vaccine, but do not make strong predictions about the roles of covariates. Using data from an ongoing cohort study, we examine these predictions and also examine whether other covariates should be included in future models. For this presentation, we focused on the effects of vaccine dose, whether the participant was vaccinated in the previous year, and the age of the participant."
  },
  {
    "objectID": "presentations/2021-EEID/index.html",
    "href": "presentations/2021-EEID/index.html",
    "title": "Exploring the effect of host factors on the relationship between pre-existing immunity and influenza vaccine response",
    "section": "",
    "text": "I presented a poster at the Ecology and Evolution of Infectious Disease conference in 2021. The conference was supposed to be held in Montpellier, France, but due to the COVID-19 pandemic was held virtually in gather.town. That was an interesting experience.\nThe poster can be found here, and is also embedded below. We are continuing our exploration of vaccine cohort data to determine what mechanistic model predictions can be verified by data, and what aspects of mechanistic models may need to be adapted."
  },
  {
    "objectID": "presentations/2023-CEIRR/index.html",
    "href": "presentations/2023-CEIRR/index.html",
    "title": "Quantifying the breadth of vaccine response with antigenic distance",
    "section": "",
    "text": "This is my talk from the 2023 CEIRR annual meeting. You can find my slides here as well as embedded at the bottom of the page.\nSummary: We’re currently working on how to quantify the “breadth” of a vaccine based on immunogenicity measurements from cohort studies. In our framework, we compute the antigenic distance of all the strains used for immune measurements and use the distance to creature antibody landscapes. From the antibody landscapes, we can compute summary statistics for the strength and breadth of the vaccine which we believe are more robust to the choice of strains used in the study than traditional methods. However, there are several methodological issues, primarily the issue of data points at the limit of detection, that we need to address to fairly test our framework. We are currently working on Bayesian hiearchical models as a strategy for mitigating the limitations of our method.\nOfficial abstract: HAI titer is a commonly used metric for assessing the immunogenicity of the seasonal influenza vaccine to the virus strains used in the formulation of the vaccine antigen. A next-generation influenza vaccine should induce protection to similar strains, but an ideal vaccine candidate would also elicit a response to other variants. An antigenic distance measurement is a way to quantify how different two variants are and can be combined with immunogenicity measurements to a panel of different viral strains following vaccination in order to assess the breadth of the vaccine response. Building on previous literature on antibody landscapes, we propose a Bayesian hierarchical modeling framework for reducing noise across individual antibody landscapes and obtain measurements of the homologous and heterologous vaccine response that take antigenic distance into account. Various weighting systems can then be applied to our measurements in order to fairly judge vaccine candidates in different scenarios. We examine the strengths and limitations of our metrics by using both generative simulations and a case study with real-world immunogenicity data."
  },
  {
    "objectID": "presentations/DIVERsity-2024/index.html",
    "href": "presentations/DIVERsity-2024/index.html",
    "title": "Quantifying the effect of HD vs SD FluZone vaccination on heterologous immunity",
    "section": "",
    "text": "I presented these slides at the DIVERsity Annual Meeting 2024. The slides are embedded below or you can get them here if you’re interested.\nYou can find a draft of our code and manuscript here: https://github.com/ahgroup/Billings-2024-HD-Heterologous."
  },
  {
    "objectID": "presentations/IDIG-2022-10-25/index.html",
    "href": "presentations/IDIG-2022-10-25/index.html",
    "title": "Journal club (Nauta et al.)",
    "section": "",
    "text": "I led a discussion of this paper\n\nNauta JJ, Beyer WE, Osterhaus AD. On the relationship between mean antibody level, seroprotection and clinical protection from influenza. Biologicals. 2009;37(4):216-221. doi:10.1016/j.biologicals.2009.02.002.\n\nat the seminar I coordinate. It was a pretty interesting paper – I read a lot about HAI titers and flu vaccines, and I think it’s interesting that I haven’t really seen a lot of people explicitly address the concerns from this paper. I also think their results extend to a lot of diseases beyond flu, and point to the general issue of “point-estimate-is-the-estimate-ism” that seems so prevalent in science. The slides are embedded below or you can get them here if you’re interested."
  },
  {
    "objectID": "presentations/Open-Science-CEID/index.html",
    "href": "presentations/Open-Science-CEID/index.html",
    "title": "Intro to Open Science, v2",
    "section": "",
    "text": "You can see the information for this talk here.\nIn January 2023, I was awarded a fellowship by the Center for Open Science, funded by FluLab. I got some excellent open science training, and one stipulation of my fellowship was that I had to share my training knowledge with my colleagues. So I talked about what open science is, why bother with open science, how to start doing open science, and briefly about the Open Science Foundation, an online platform developed by OSF to empower open science workflows. The slides are embedded below or you can get them here if you’re interested.\n\n\nNote that, as an exception to the rest of my website, this slide set is released under the CC BY 4.0 license, rather than the CC BY-NC-SA 4.0 license.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "presentations/REU-Workshop2023/index.html",
    "href": "presentations/REU-Workshop2023/index.html",
    "title": "Reproducible research workshop",
    "section": "",
    "text": "In June 2023, I led a workshop on reproducible research for the Population Biology of Infectious Diseases REU site at the University of Georgia.. The slides are embedded below or you can get them here if you’re interested."
  },
  {
    "objectID": "posts/2022-06-24_Bayes-Diagnostics/index.html",
    "href": "posts/2022-06-24_Bayes-Diagnostics/index.html",
    "title": "Multiple diagnostic testing and Bayes’ rule",
    "section": "",
    "text": "Two negative Abbot BinaxNOW Covid-19 antigen test cards on a dark wooden table, next to a kitchen timer shaped like a chicken.\nPic related: this morning a coworker let me know they had tested positive for COVID-19! As someone who actually prefers to go into the office, I knew that this was one of the dangers, so I was at least prepared for this to happen. I grabbed my government-mailed COVID-19 rapid tests from the closet, checked the expiration date, sat down at the table, and…remembered that flu rapid tests have high false positive rates. Admittedly, I know much more about flu diagnostic testing than COVID.\nThe package insert for the Abbott BinaxNOW tests alleges a 91.7% sensitivity (95% CI: 73.0% - 98.9%), but with the 73% lower bound and my skeptical nature, I decided I better go ahead and do two tests. Of course, being a math degree holder, I can certainly work out for myself how to use Bayes’ Rule to update my beliefs about my COVID status after seeing two tests, but I decided to google it to see if there are any quick and easy guides. And of course, most of what I found in the top google results were articles that didn’t really have much content. So, given the advice I’ve recieved from Andreas Handel and Andrew Heiss, I decided to write up my very first blog post about this subject. Finally, a subject where I feel confident enough to write a blog post about it."
  },
  {
    "objectID": "posts/2022-06-24_Bayes-Diagnostics/index.html#diagnostic-test-sensitivity-and-specificity",
    "href": "posts/2022-06-24_Bayes-Diagnostics/index.html#diagnostic-test-sensitivity-and-specificity",
    "title": "Multiple diagnostic testing and Bayes’ rule",
    "section": "Diagnostic test sensitivity and specificity",
    "text": "Diagnostic test sensitivity and specificity\nEvery diagnostic test that produces a dichotomous outcome (for example, COVID-19 positive vs. COVIC-19 negative) has some margin of error. When the test gives you a positive diagnosis, it never means a 100% chance that you are positive. Instead, the test has a probability of being positive, given that you actually have the disease. This probability is called the sensitivity of the test. In math notation, we would write \\[\\mathrm{sensitivity} = P\\left(+ \\mid D\\right)\\] where \\(D\\) is the event that you have the disease (we’ll call the event that you don’t have the disease \\(\\lnot D\\)) and \\(+\\) is the event that you have a positive test (we’ll call the event you have a negative test \\(-\\)). Note that there are only four possibilities with this kind of test.\n\nTrue positive: You actually have the disease, and you test positive.\nFalse positive: You don’t have the disease, but you test positive.\nFalse negative: You actually have the disease, but you test negative.\nTrue negative: You don’t have the disease, and you test negative.\n\nUsing these definitions, the sensitivity of the test is also called the “true positive rate”. Similarly, there is another metric for the test called the specificity, which is called the “true negative rate” and represents the probability that you test negative, given that you do not have the disease. Mathematically, we would write \\[\\mathrm{specificity} = P(- \\mid \\lnot D).\\]\nThese quantities are intrinsic to the test. Theoretically, every type of diagnostic test has a true sensitivity and specificity that we can estimate by using the test on people whose disease status we know. In the US, the companies that make these tests are required to do these studies before their products can be approved by the FDA.\nFor the rest of this blog post, we’ll use the slightly more conservative numbers provided in the package insert: in a multi-site clinical study, the test was estimated to have a sensitivity of 85% and a specificity of 99% (rounding to the nearest percentage).\nHowever, the sensitivity and specificity don’t tell us the information that we actually want to know. What I want to know is, given that I tested negative, what is the probability that I do not have COVID-19?"
  },
  {
    "objectID": "posts/2022-06-24_Bayes-Diagnostics/index.html#bayes-rule-and-the-ppvnpv",
    "href": "posts/2022-06-24_Bayes-Diagnostics/index.html#bayes-rule-and-the-ppvnpv",
    "title": "Multiple diagnostic testing and Bayes’ rule",
    "section": "Bayes’ Rule and the PPV/NPV",
    "text": "Bayes’ Rule and the PPV/NPV\nIn math notation, the quantity that we want to calculate is \\[P(D \\mid +),\\] which is sort of the opposite of the sensitivity of the test. Fortunately, statistics has a convenient way to “flip” the conditional probability: Bayes’ Rule. \\[P(D \\mid +) = \\frac{P(+ \\mid D)P(D)}{P(+)} = \\frac{\\mathrm{sensitivity} \\cdot \\mathrm{prevalence}}{\\text{probability of testing positive}}\\]\nSo, you see that there are two additional components here that we currently don’t know: the prevalence of the disease, which we call \\(P(D)\\), and the probability of testing positive on the test, \\(P(+)\\).\nWe can use another trick to get \\(P(+)\\) called the law of total probability: \\[P(+) = P(+ \\mid D) P(D) + P(+ \\mid \\lnot D) P(\\lnot D).\\]\nUsing some math, we can rewrite this. We know that \\(P(+ \\mid D)\\) is the sensitivity, or true positive rate, so we can rease that \\(P(+ \\mid \\lnot D)\\) is the true negative rate. Additionally, since we either have or don’t have the disease (there is no other outcome), we know that \\(P(\\lnot D) = 1 - P(D)\\), so we get \\[P(+) = \\text{TPR} \\cdot P(D) + \\text{FPR} \\cdot (1 - P(D)).\\]\nWe can get the false positive rate from the specificity by noting that, given you have a negative test, you must either have the disease or not (there are only these two options), so then \\[1 = P(+ \\mid D) + P(+ \\mid \\lnot D) = \\mathrm{TPR} + \\mathrm{FPR}.\\]\nTherefore, the false positive rate is \\(\\mathrm{TPR} = 1 - 0.99 = 0.01\\). Now we just need to know the prevalence of disease. Of course, I had issues getting the Georgia Department of Public Health’s tracker to work, but when I checked the CDC’s COVID data tracker, there was a reported rate of 150.39 cases per 100,000 population, which works out to a prevalence percentage (cases per 100 people) of 0.15% or 0.0015.\nSo finally we can work out that \\[P(+) = (0.85)(0.0015) + (0.01)(0.9985) = 0.00128 + 0.00999 \\approx 0.01270,\\] or about \\(1.3\\%\\). (For Athens-Clarke County, Georgia, on June 24, 2022.) Now that we have estimated the probability of a positive test in general, we can compute the value we actually want, which as I mentioned is called the positive predictive value.\nSo, given that we have a positive test, the probability of actually having COVID would be \\[\\mathrm{PPV} = P(D \\mid +) = \\frac{P(+ \\mid D)P(D)}{P(+)} = \\frac{0.85\\cdot 0.0015}{0.0127} \\approx 0.1004 = 10.04\\%.\\] That might seem low, but of course your risk of having COVID (your personal \\(P(D)\\)) increases drastically if you’ve been around someone else who tested positive. Later on, I’ll vary this number so we can see this effect in action.\nConversely, since you’ve already seen that I tested negative, we might want the negative predictive value: the probability of actually being negative, given that the test was negative. We compute this similarly, so I won’t walk through all the steps this time. We have\n\\[\\mathrm{NPV} = P(\\lnot D \\mid -) = \\frac{P(- \\mid \\lnot D)P(\\lnot D)}{P(-)} = \\frac{P(- \\mid \\lnot D)P(\\lnot D)}{P(- \\mid \\lnot D)P(\\lnot D) + P(- \\mid D)P(D)},\\] which works out mathematically in the same way as the PPV, and we get \\[\\mathrm{NPV} = \\frac{0.99 \\cdot 0.9985}{(0.99 \\cdot 0.9985) + (0.15\\cdot0.0015)} \\approx 0.9998 \\approx 99.8\\%.\\] Again, remember that this is heavily dependent on what we chose as the prevalence."
  },
  {
    "objectID": "posts/2022-06-24_Bayes-Diagnostics/index.html#last-updated",
    "href": "posts/2022-06-24_Bayes-Diagnostics/index.html#last-updated",
    "title": "Multiple diagnostic testing and Bayes’ rule",
    "section": "Last updated",
    "text": "Last updated\n\n2022-06-24 12:50:12 EDT"
  },
  {
    "objectID": "posts/2022-06-24_Bayes-Diagnostics/index.html#details",
    "href": "posts/2022-06-24_Bayes-Diagnostics/index.html#details",
    "title": "Multiple diagnostic testing and Bayes’ rule",
    "section": "Details",
    "text": "Details\n\nsource code\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19043)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.6 dplyr_1.0.10 \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1     compiler_4.2.1   tools_4.2.1      digest_0.6.29   \n [5] jsonlite_1.8.0   evaluate_0.16    lifecycle_1.0.2  tibble_3.1.8    \n [9] gtable_0.3.1     pkgconfig_2.0.3  rlang_1.0.5      cli_3.4.1       \n[13] DBI_1.1.3        rstudioapi_0.14  yaml_2.3.5       xfun_0.32       \n[17] fastmap_1.1.0    withr_2.5.0      stringr_1.4.1    knitr_1.40      \n[21] generics_0.1.3   vctrs_0.4.2      grid_4.2.1       tidyselect_1.1.2\n[25] glue_1.6.2       R6_2.5.1         fansi_1.0.3      rmarkdown_2.16  \n[29] farver_2.1.1     purrr_0.3.4      magrittr_2.0.3   scales_1.2.1    \n[33] htmltools_0.5.3  assertthat_0.2.1 colorspace_2.0-3 renv_0.15.5     \n[37] labeling_0.4.2   utf8_1.2.2       stringi_1.7.8    munsell_0.5.0"
  },
  {
    "objectID": "posts/2023-10-28_TI-vs-Post-Regression/index.html",
    "href": "posts/2023-10-28_TI-vs-Post-Regression/index.html",
    "title": "Increase vs post titer as regression outcome",
    "section": "",
    "text": "For a vaccine study with an immunogenicity endpoint, two commonly used outcomes are the raw post-vaccination titer, or the fold change in titers between the post-vaccination and pre-vaccination endpoint. Both provide interesting information. However, under many of the simple statistical models that are used for both outcomes, the model results are deterministically related, which I want to show here."
  },
  {
    "objectID": "posts/2023-10-28_TI-vs-Post-Regression/index.html#generative-model",
    "href": "posts/2023-10-28_TI-vs-Post-Regression/index.html#generative-model",
    "title": "Increase vs post titer as regression outcome",
    "section": "Generative model",
    "text": "Generative model\nFirst, we generate an example dataset. This dataset will have two columns, just pre-titer and post-titer with no other confounders. Here’s the population parameters. Throughout this example, I will assume all effects for the titer values occur on the log scale for simplicity. Though we could have a lot of arguments about the generative model, I specifically chose a very simple and easy to work with generative model that I think illustrates the point that I want to make.\n\nsim_parms &lt;- list(\n    S &lt;- 32482304,\n    N &lt;- 1000,\n    alpha &lt;- 3,\n    beta &lt;- 0.5,\n    pre_mean &lt;- 2,\n    pre_var &lt;- 2,\n    error_var &lt;- 2\n)\nstr(sim_parms)\n\nList of 7\n $ : num 32482304\n $ : num 1000\n $ : num 3\n $ : num 0.5\n $ : num 2\n $ : num 2\n $ : num 2\n\n\nNow we generate the data. Even though HAI titers always have the \\(\\times 5\\) in the formulas, I left that off here for simplicity. Since it’s a constant multiplier it doesn’t affect what’s going on here.\n\ngen_data &lt;- function(S, N, alpha, beta, pre_mean, pre_var, error_var) {\n    set.seed(S)\n    dat &lt;- tibble::tibble(\n        log_pre = rnorm(N, pre_mean, pre_var),\n        pre = (2 ^ log_pre),\n        noise = rnorm(N, 0, error_var),\n        log_post = alpha + beta * log_pre + noise,\n        post = (2 ^ log_post),\n        TI = log_post - log_pre,\n        FC = 2 ^ TI\n    )\n    return(dat)\n}\nsim_dat &lt;- do.call(gen_data, sim_parms)\nprint(sim_dat, n = 5)\n\n# A tibble: 1,000 × 7\n  log_pre     pre  noise log_post  post    TI     FC\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1   6.95  124.    -1.72      4.75  27.0 -2.20  0.218\n2  -0.501   0.707  1.83      4.58  23.9  5.08 33.8  \n3   2.02    4.04   0.706     4.71  26.2  2.70  6.49 \n4   0.832   1.78   1.82      5.23  37.6  4.40 21.1  \n5   5.91   60.0   -1.23      4.72  26.4 -1.19  0.439\n# … with 995 more rows\n\n\nIf we plot the pre-titers vs the post-titers, we see a cloud of points around the regression line, as we would expect.\n\nggplot(sim_dat) +\n    aes(x = pre, y = post) +\n    geom_point() +\n    geom_abline(slope = beta, intercept = alpha, color = \"red\") +\n    scale_x_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2)) +\n    scale_y_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2))\n\n\n\n\n\n\n\n\nThe relationship between titer increase and pretiter is also completely determined by this linear model. For the specific set of parameters I chose for the simulation, we see a negative effect of pre-titer on titer increase (which we think is true in real life), but with this generative model, this is entirely dependent on the value of \\(\\beta\\). Because I set a value of \\(\\beta\\) which is in \\((0, 1)\\), we observe a positive correlation between pre and post- titer, and a negative correlation between pre-titer and titer increase. We will see why shortly.\n\nggplot(sim_dat) +\n    aes(x = pre, y = FC) +\n    geom_point() +\n    scale_x_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2)) +\n    scale_y_continuous(trans = \"log2\", breaks = scales::breaks_log(base = 2))"
  },
  {
    "objectID": "posts/2023-10-28_TI-vs-Post-Regression/index.html#regression-offsets",
    "href": "posts/2023-10-28_TI-vs-Post-Regression/index.html#regression-offsets",
    "title": "Increase vs post titer as regression outcome",
    "section": "Regression offsets",
    "text": "Regression offsets\nWe can derive the regression line for the case where the titer increase is the outcome of interest.\n\\[\n\\begin{aligned}\n\\log_2(\\text{post}) &= \\alpha + \\beta \\cdot \\log_2(\\text{pre}) \\\\\n\\log_2(\\text{post}) - \\log_2(\\text{pre}) &= \\alpha + \\beta \\cdot \\log_2(\\text{pre}) - \\log_2(\\text{pre})\\\\\n\\text{titer increase} &= \\alpha + (\\beta - 1) \\cdot \\log_2 (\\text{pre})\n\\end{aligned}\n\\] That is, we can include the negative pre-titer as an offset term in the linear model for titer increase to recover the same coefficients as we would in the model for post-titer. This is why we got the specific qualitative behavior we observed earlier.\nNote that a regression offset is a term that is included on the right-hand side of a linear model, but the coefficient is pre-specified as 1, and not estimated as part of the model fit.\n\nmod1 &lt;- lm(log_post ~ log_pre, data = sim_dat)\nmod2 &lt;- lm(TI ~ log_pre, data = sim_dat)\nmod3 &lt;- lm(TI ~ log_pre, data = sim_dat, offset = (-1 * log_pre))\n\nHere we can see that model 1 and model 3 recover the parameters from the generative model, while if we fit model 2, we get a biased coefficient that is exactly equal to 1 minus the estimated parameter from the other two models. So while both models produce a “correct” inference, we need to be aware why the models are different.\n\ndplyr::bind_rows(\n    \"mod1\" = coef(mod1),\n    \"mod2\" = coef(mod2),\n    \"mod3\" = coef(mod3),\n    .id = \"model\"\n)\n\n# A tibble: 3 × 3\n  model `(Intercept)` log_pre\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 mod1           3.05   0.479\n2 mod2           3.05  -0.521\n3 mod3           3.05   0.479\n\n\nNote that adding linear confounders will not change what happens here. For example, we could add \\(\\beta_2 \\cdot \\mathrm{age}\\) into our generative model and our regression models, and we would see the same pattern. However, if we incorporate nonlinearity (e.g. interaction effects or hierarchical effects), understanding how the models are related might not be that simple to solve, because in those cases we can’t necessarily combine the offset term with something else to simplify the model."
  },
  {
    "objectID": "posts/2023-10-28_TI-vs-Post-Regression/index.html#other-outcomes",
    "href": "posts/2023-10-28_TI-vs-Post-Regression/index.html#other-outcomes",
    "title": "Increase vs post titer as regression outcome",
    "section": "Other outcomes",
    "text": "Other outcomes\nIf we were to construct an outcome that is not linear in the pre-titer value, this equivalence would no longer hold. For example, if instead of using the log fold change as the outcome, we instead used\n\\[y = \\frac{\\log_2 (\\text{post})}{\\log_2 (\\text{pre})},\\] the models are no longer equivalent in this way. This model is not commonly used because the ratio of the logarithms is not as easily interpreted as the log fold change, but we could do that if we wanted. If we fit a model with that outcome we would get \\[\n\\begin{aligned}\n\\frac{\\log_2 (\\text{post})}{\\log_2 (\\text{pre})} &=\n\\frac{\\alpha + \\beta\\cdot\\log_2 (\\text{pre})}{\\log_2 (\\text{pre})} \\\\\n&=\\beta + \\alpha\\frac{1}{\\log_2 (\\text{pre})}.\n\\end{aligned}\n\\]\n\nx &lt;- 1 / sim_dat$log_pre\ny &lt;- sim_dat$log_post / sim_dat$log_pre\nmod4 &lt;- lm(y ~ sim_dat$log_pre)\nmod5 &lt;- lm(y ~ x)\ndplyr::bind_rows(\n    \"mod4\" = coef(mod4),\n    \"mod5\" = coef(mod5),\n    .id = \"model\"\n)\n\n# A tibble: 2 × 4\n  model `(Intercept)` `sim_dat$log_pre`      x\n  &lt;chr&gt;         &lt;dbl&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1 mod4          1.30              0.177 NA    \n2 mod5          0.899            NA      0.916\n\n\nUh-oh, those aren’t the numbers I said we should get! Actually, a major problem with this model is that those transformations kind of wreck our numeric precision. If we plot the transformed data, we can see that regression line still passes through the middle of the data really well, we just get some crazy points that ruin our ability to estimate this with a plain linear regression model.\n\nplot(x, y)\nabline(a = beta, b = alpha)\n\n\n\n\n\n\n\n\nSo I guess that’s probably another reason this outcome doesn’t get used much."
  },
  {
    "objectID": "posts/2023-10-28_TI-vs-Post-Regression/index.html#details",
    "href": "posts/2023-10-28_TI-vs-Post-Regression/index.html#details",
    "title": "Increase vs post titer as regression outcome",
    "section": "Details",
    "text": "Details\n\nLast updated at 2023-10-28 22:18:34.847873.\nsource code\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.6\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.5.0       cli_3.4.1         knitr_1.40        rlang_1.0.6      \n [5] xfun_0.34         stringi_1.7.8     generics_0.1.3    renv_0.16.0      \n [9] jsonlite_1.8.3    glue_1.6.2        colorspace_2.0-3  htmltools_0.5.3  \n[13] fansi_1.0.3       scales_1.2.1      rmarkdown_2.17    grid_4.3.1       \n[17] evaluate_0.17     munsell_0.5.0     tibble_3.1.8      fastmap_1.1.0    \n[21] yaml_2.3.6        lifecycle_1.0.3   zlib_0.0.1        stringr_1.4.1    \n[25] compiler_4.3.1    dplyr_1.0.10      pkgconfig_2.0.3   htmlwidgets_1.5.4\n[29] farver_2.1.1      digest_0.6.30     R6_2.5.1          tidyselect_1.2.0 \n[33] utf8_1.2.2        pillar_1.8.1      magrittr_2.0.3    withr_2.5.0      \n[37] tools_4.3.1       gtable_0.3.1"
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html",
    "href": "posts/2023-10-09_BootstrapIntro/index.html",
    "title": "Intro to Bootstrap CIs",
    "section": "",
    "text": "In this blog post, I’m going to go over a short example of using the nonparametric bootstrap to construct confidence intervals for arbitrary statistics. Exact and asymptotic limits are all great, but unfortunately they have to exist before you can calculate them, and these methods are often very analytically difficult (requiring, e.g., Taylor series analysis). We can often trade off computational power for analytical problem solving using the bootstrap method to construct pretty good confidence intervals instead.\nI’ll start by going over what I think is the most intuitive approach for bootstrap CIs, the percentile method. This method has some limitations though, so I’ll talk about the BCa adjustment which can produce better results, and I’ll talk about how to get these CIs with the boot package in R."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#example-problem-setup",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#example-problem-setup",
    "title": "Intro to Bootstrap CIs",
    "section": "Example problem setup",
    "text": "Example problem setup\nFor an example, we’ll make some simulated data that’s similar to the situation where we actually need to use a bootstrap CI in my lab group’s research. We’ll consider a fairly simple example: in a study sample of size \\(n = n_t + n_p\\), where \\(n_t\\) people get a vaccine and \\(n_p\\) people get a placebo. They are followed up for an appropriate amount of time (or challenge with virus, your choice), and we record which individuals get infected and which do not. For this simulated example, the effect we are interested in measuring is the vaccine effectiveness, which can be calculated from the risk ratio. That is, \\[\\mathrm{VE} = 1 - \\mathrm{RR} = 1 - \\frac{\\text{risk in vaccine group}}{\\text{risk in placebo group}}.\\] To do this, we’ll simulate infection data for both groups. If we want our target VE to be, say, \\(75\\%\\), we want the risk ratio to be \\(25\\%\\), and we can set the two prevalences however we want, as long as they are sensible prevalences (between \\(0\\%\\) and \\(100\\%\\)) and their ratio is \\(\\frac{1}{4}\\). So for ease of computation, let’s say that the risk in the vaccine group is \\(10\\%\\) and the risk in the placebo group is \\(40\\%\\). Let’s simulate 100 patients where 50 are in the placebo group and 50 are in the vaccine group.\n\nset.seed(134122)\nn &lt;- 100L\nsim_dat &lt;- tibble::tibble(\n    # Treatment is 0 for placebo, 1 for vaccinated\n    treatment = rep(c(0, 1), each = n / 2),\n    outcome = rbinom(n, size = 1, prob = rep(c(0.4, 0.1), each = n / 2))\n)\n\ntable(\n    sim_dat$treatment,\n    sim_dat$outcome,\n    dnn = c(\"treatment\", \"outcome\")\n) |&gt;\n    addmargins()\n\n         outcome\ntreatment   0   1 Sum\n      0    27  23  50\n      1    43   7  50\n      Sum  70  30 100\n\n\nIf we look at our table, we see that we get estimated risks of \\(23/50 = 46\\%\\) in the placebo group, and \\(7/50 = 14\\%\\) in the vaccine group, giving us an observed VE of \\(1 - 0.3043 \\approx 70\\%.\\) This is close to what we know is the true VE in the simulation, but different enough that we can immediately see the effect of sampling variability. Of course, as we simulate increasingly more people, we will get a more accurate estimate of the VE.\nNow, we might want to construct a confidence interval for our sample, which would ideally cover the true VE. But we forgot all the different methods for building confidence intervals for the risk ratio, no matter how many of them they taught us or how many times we covered it during epidemiology class. (Or if you’re like me, you did an epidemiology PhD without any intro to epi classes and never really learned…) So fortunately for us, nonparametric bootstrapping provides a really convenient and flexible way to “automatically” get a CI, as long as we have a relatively modern computer – even though bootstrapping is called a computer intensive method, for simple statistics like this one, you can get bootstrap intervals pretty fast even on cheap hardware."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#constructing-resamples",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#constructing-resamples",
    "title": "Intro to Bootstrap CIs",
    "section": "Constructing resamples",
    "text": "Constructing resamples\nThe first step of computing a bootstrap CI is to construct the bootstrap distribution. Bootstrapping refers to a specific type of resampling process that’s similar to cross-validation or leave-one-out methods. Call our entire data set \\(D\\). We construct a bootstrap resample \\(D_i\\) (\\(i = 1, 2, \\ldots, B\\) where \\(B\\) is the number of bootstrap resamples we do) by sampling with replacement \\(n\\) records from \\(D\\). This means that each resample can have the same observation repeated multiple times, and does not have to include every observation (in fact, we can use the bootstrap similarly to cross-validation for model tuning by using the bootstrap resample as the analysis set and the points not included in the resample as the assessment set). We resample in this way specifically, because it leads to specific properties (under certain regularity conditions) which allow our confidence intervals to work correctly. (Math details omitted since they’re complicated and honestly not too important for what we’re trying to do here.)\nWe could of course do this by hand.\n\nset.seed(3547512)\nB &lt;- 199L\nres &lt;- matrix(nrow = B, ncol = 4)\ncolnames(res) &lt;- c(\"p_p\", \"p_t\", \"RR\", \"VE\")\nfor (i in 1:B) {\n    # Sample the data indices that we should use for this sample\n    idx &lt;- sample.int(nrow(sim_dat), replace = TRUE)\n    D_i &lt;- sim_dat[idx, ]\n    # Code to calculate the risk in the placebo group and the risk in the\n    # treatment group\n    # We could write this in a much more compact/efficient way, but I did it\n    # this way for readability\n    p_p &lt;- D_i |&gt;\n        dplyr::filter(treatment == 0) |&gt;\n        dplyr::pull(outcome) |&gt;\n        mean()\n    p_t &lt;- D_i |&gt;\n        dplyr::filter(treatment == 1) |&gt;\n        dplyr::pull(outcome) |&gt;\n        mean()\n    RR &lt;- p_t / p_p\n    \n    # Store results in output matrix\n    res[i, 1] &lt;- p_p\n    res[i, 2] &lt;- p_t\n    res[i, 3] &lt;- RR\n    res[i, 4] &lt;- 1 - RR\n}\n\nres &lt;- tibble::as_tibble(res)\nprint(res, n = 6)\n\n# A tibble: 199 × 4\n    p_p    p_t    RR    VE\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.544 0.0698 0.128 0.872\n2 0.490 0.196  0.400 0.600\n3 0.46  0.12   0.261 0.739\n4 0.467 0.15   0.321 0.679\n5 0.533 0.145  0.273 0.727\n6 0.341 0.125  0.367 0.633\n# … with 193 more rows\n\n\nThis gives us a resampling or bootstrap distribution with 199 values for each of the calculated values (\\(p_p\\), \\(p_t\\), the \\(\\mathrm{RR}\\), and the \\(\\mathrm{VE}\\)). We can look at those distributions. (I’ll briefly explain why we might choose 199 in a moment.)\n\nres |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything()) |&gt;\n    ggplot2::ggplot() +\n    ggplot2::aes(x = value) +\n    ggplot2::geom_histogram(\n        binwidth = 0.05,\n        boundary = 0,\n        color = \"black\",\n        fill = \"gray\"\n    ) +\n    ggplot2::facet_wrap(ggplot2::vars(name)) +\n    ggplot2::scale_x_continuous(\n        breaks = seq(0, 1, 0.1),\n        minor_breaks = seq(0, 1, 0.05),\n        labels = seq(0, 1, 0.1),\n        limits = c(0, 1)\n    )\n\n\n\n\n\n\n\n\nHere we only do 199 resamples because the program I wrote is super inefficient and doing much more would take a while. (Although it is worthwhile to note that the calculation of bootstrap statistics is trivially parallelizable, so with access to sufficient resources, even a computationally expensive or poorly implemented statistic can be computed in a reasonable amount of time.)\nSo now that we have calculated a bootstrap distribution for our statistic of interest (the VE, along with its component statistics as recordkeeping), how do we estimate the CI from this distribution? In my opinion, the most intuitive way is to use the percentile method."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#percentile-method",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#percentile-method",
    "title": "Intro to Bootstrap CIs",
    "section": "Percentile method",
    "text": "Percentile method\nBefore introducing the percentile method, we should note that many people on the internet will complain and moan about how “bad” this method is, including Davison and Hinkley (who did it in their book that is older than stack exchange or whatever). People like to moan about how it is not quite right, but as Efron (bootstrap inventor and expert) and Hastie put it, the goal of bootstraps is to have wide applicability and close to perfect coverage. So this method is, in general, pretty good in a wide variety of cases. Especially if we think about all the errors that get compounded in the process of observing and analyzing real-world data, something as trivial as being a few coverage percentage points off from \\(95\\%\\) is really not something to cry about, especially when we can construct intervals for arbitrarily complicated statistics. So, let’s define the method.\nThe percentile method indeed is so simple that skepticism is natural. Whereas other bootstrap methods rely on finding normalization transformations to ensure correct confidence limits, the percentile method relies only on the existence of such a transformation, which can be unknown to us, as this method is scale-invariant. This method literally just involves calculating the closest empirical quantiles of interest to those we are interested in. Using the percentile method, a \\(95\\%\\) confidence interval for the VE would be \\[\\left( \\mathrm{VE}^{*}_{(B+1)(0.025)}, \\mathrm{VE}^{*}_{(B+1)(0.975)} \\right)\\]\nSo if we chose \\(B=199\\), then we would order our observed statistics, and our percentile method CI would be formed by the 5th entry in that vector and the 195th entry in that vector. For such ease of computation, \\(B = 399\\) or \\(599\\) and so on are often used to ensure the quantiles are exact. However, again, this is an example of sweating over tiny amounts of CI coverage that likely are not too important to begin with, so choosing a nice round number like \\(B = 2000\\) is often sufficient. Typically, you need at least \\(B = 1000\\), though \\(B = 10000\\) (or \\(9999\\ldots\\)) is even better and typically not even too computationally demanding on modern machines. So, for our example, our percentile CI for the VE would be calculated as follows.\n\nsort(res$VE)[c(5, 195)]\n\n[1] 0.4441367 0.9032258\n\n\nR has a lot of quantile algorithms, but we can specifically use Type 1 or 2 to get the same estimates (2 is probably better if you don’t use a \\(B\\) ending in 9’s). The others are all fairly similar though and like I said, I really don’t think being upset about the amount of difference you get between quantile algorithms really matters.\n\nquantile(res$VE, probs = c(0.025, 0.975), type = 1)\n\n     2.5%     97.5% \n0.4441367 0.9032258 \n\n\nSo, we would pair this with our empirical estimate of the VE (we could take, e.g. the mean of the bootstraps, which should always be similar to the overall estimate we already got) to get an estimated VE of \\[\\widehat{VE} = 69.57\\% \\\n\\left(95\\% \\text{ CI: } 44.41\\% \\ - \\ 90.32\\%\\right).\\] Now that is maybe not the most precise estimate of VE in the world, but based on the bootstrap percentile method, it at least seems that our vaccine is conferring some protection, with the possibility of a pretty good amount. And the CI covers our true estimate, as we would hope (although it’s important to remember that we could’ve gotten unlucky with the random sample we drew, and if our CI didn’t cover the true estimate on one experiment, that doesn’t mean the CI is flawed). Note that we would expect this CI to get smaller as \\(n\\) increases, but not as \\(B\\) increases. Increasing \\(B\\) decreases the “Monte Carlo” (MC) error, or random error introduced by the random process involved in the algorithm, but doesn’t do anything about the actual sampling error.\nOf course, like I said, some people really don’t like the percentile method. So we can instead appeal to a much more complicated method devised by Efron and some coworkers called the BCa method, and I’ll show how to automate the calculation of these CIs."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#bca-method-and-boot",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#bca-method-and-boot",
    "title": "Intro to Bootstrap CIs",
    "section": "BCa method and boot",
    "text": "BCa method and boot\nThe bias-corrected and accelerated (BCa) percentile method introduces, as you might guess, new complications to reduce the bias and accelerate the convergence of the percentile bootstrap method. Davison and Hinkley agree with Efron and Hastie that this method fixes many of the issues with the basic percentile CI, and there are not really any major objections in the same way that there are for the percentile method. However, this method can be more computationally intensive and certainly more demanding to calculate, and as such I do not recommend implementing in by hand in everyday situations. The central problem here is the estimation of the acceleration parameter \\(a\\), although there is fortunately a nonparametric estimator that works well in many situations.\nSo, we will use the boot package.\n\nlibrary(boot)\n\nThe first step to getting bootstrap CIs via boot is to write a function with a specific order of arguments. The first argument passed to this function will be the original data, and the second will be a vector of indices which are sampled by the bootstrapping procedure. Additional arguments are allowed. For our problem, a function might look like this.\n\nest_VE &lt;- function(data, idx) {\n    # Sample the data indices that we should use for this sample\n    D_i &lt;- data[idx, ]\n    # Code to calculate the risk in the placebo group and the risk in the\n    # treatment group\n    p_p &lt;- mean(subset(D_i, treatment == 0)$outcome)\n    p_t &lt;- mean(subset(D_i, treatment == 1)$outcome)\n    RR &lt;- p_t / p_p\n    \n    # Return the VE\n    return(1 - RR)\n}\n\nNext, we need to create a resampling object using the boot::boot() function. I’ll comment the important arguments in the code chunk. There are many other arguments we could pass in, and the details of those are described in the manual or in Davison and Hinkley (see references). In particular, the parallel, cl, and ncpus arguments can be used for easy parallel computing.\n\nset.seed(7071234)\nVE_boot &lt;- boot::boot(\n    # First argument is the data frame\n    data = sim_dat,\n    # Second argument is the function we want to calculate on resamples\n    statistic = est_VE,\n    # R is the number of resamples, which I called B before\n    R = 9999,\n    # We want to use ordinary nonparametric bootstrap -- this is the default\n    # but I wanted to emphasize it\n    sim = \"ordinary\"\n)\n\nBecause the boot package is optimized so well, that seems like it actually took less time than my simple example earlier. If we just print the object, we can see the estimated statistic and bootstrap SE.\n\nVE_boot\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot::boot(data = sim_dat, statistic = est_VE, R = 9999, sim = \"ordinary\")\n\n\nBootstrap Statistics :\n     original     bias    std. error\nt1* 0.6956522 -0.0074171   0.1223473\n\n\nWe can also plot the bootstrap distribution.\n\nplot(VE_boot)\n\n\n\n\n\n\n\n\nWe can see that it is nonnormal, with a long left tail. Such nonnormality indicates that the normal approximate bootstrap method (which I didn’t discuss) is not appropriate, and the BCa method is generally much better in this case.\nThere is a lot you can do with this package that I am not going to talk about and don’t really know about. But the next thing we want to do for this example is compute those BCa confidence intervals. Fortunately for us, we can actually calculate all the types of CI available with one function call. Note that we could also compute the “studentized” CI, but that requires us to specify variances for each observation.\n\nVE_ci &lt;- boot::boot.ci(\n    VE_boot, conf = 0.95,\n    type = c(\"norm\", \"basic\", \"perc\", \"bca\")\n)\nVE_ci\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 9999 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = VE_boot, conf = 0.95, type = c(\"norm\", \n    \"basic\", \"perc\", \"bca\"))\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.4633,  0.9429 )   ( 0.4991,  0.9807 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.4106,  0.8922 )   ( 0.3701,  0.8778 )  \nCalculations and Intervals on Original Scale\n\n\nWe can see that the percentile CI is quite similar to the one we got by hand, which is nice and the difference is likely attributable to random error. Notice, however, that the normal and basic intervals are shifted upwards from the percentile interval, and the BCa interval is shifted down. The BCa interval is corrected for bias, which is why it is shifted down (in Efron and Hastie they discuss why our CIs might be biased upwards to begin with). In this case, the different intervals are actually a bit different, although again I’m not sure how useful it is to fret over whether the lower limit of the CI for our VE is \\(41\\%\\) or \\(37\\%\\). (But I do admit that a range of \\(46\\%\\) to \\(37\\%\\) across methods is not ideal, though I don’t know that these two estimates would change any qualitative decisions that you might make based on the analysis. But either way the normal method is not great, and I don’t love the basic method, which is why I didn’t recommend using either of those.)\nIn general I would use the BCa limits, since they are the least controversial, technically the “best” in a specific sense, and are just as easy to calculate. And it’s important to note that all of these CIs cover our true parameter for this simulated experiment; to truly investigate the convergence and bias properties of these different CIs we would need to run many simulated experiments."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#conclusions",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#conclusions",
    "title": "Intro to Bootstrap CIs",
    "section": "Conclusions",
    "text": "Conclusions\nIn this tutorial, we simulated data and got confidence intervals using two different bootstrap methods for the VE from a theoretical vaccine study. We demonstrated how easy it is to obtain BCa intervals using the boot package in R.\nWe should note that bootstrapping does not solve any issues induced by having too low of a sample size – bootstrap CIs will often be just as bad as any other CIs if the sample size is quite small. If I were to keep writing this blog post, I would compute several of the other alternative CIs for the VE, like the normal approximate and whatever exact CIs people are using for that right now, I just couldn’t think of those off the top of my head and I don’t think comparing to only the normal approximate CI is useful.\nThere are some limitations to bootstrap, especially when we start using complex hierarchical models. But for simple CI calculations, nonparametric BCa bootstraps can often provide a decent CI estimate without requiring any analytical derivations."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#references",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#references",
    "title": "Intro to Bootstrap CIs",
    "section": "References",
    "text": "References\n\nDavison AC and Hinkley DV. Bootstrap Methods and their Applications, 1997. Cambridge University Press, Cambridge. ISBN 0-521-57391-2.\nEfron B and Hastie T. Computer Age Statistical Inference, student edition, 2021. Cambridge University Press, Cambridge.\nRousselet GA, Pernet CR, Wilcox RR. The Percentile Bootstrap: A Primer With Step-by-Step Instructions in R. Advances in Methods and Practices in Psychological Science. 2021;4(1). doi:10.1177/2515245920911881.\nCanty A and Ripley B (2022). boot: Bootstrap R (S-Plus) Functions. R package version 1.3-28.1."
  },
  {
    "objectID": "posts/2023-10-09_BootstrapIntro/index.html#details",
    "href": "posts/2023-10-09_BootstrapIntro/index.html#details",
    "title": "Intro to Bootstrap CIs",
    "section": "Details",
    "text": "Details\n\nLast updated at 2023-10-11 09:28:58.632938.\nsource code\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] boot_1.3-28.1 ggplot2_3.3.6\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.1      jsonlite_1.8.3    dplyr_1.0.10      compiler_4.3.1   \n [5] renv_0.16.0       tidyselect_1.2.0  stringr_1.4.1     tidyr_1.2.1      \n [9] scales_1.2.1      yaml_2.3.6        fastmap_1.1.0     R6_2.5.1         \n[13] labeling_0.4.2    generics_0.1.3    knitr_1.40        htmlwidgets_1.5.4\n[17] tibble_3.1.8      munsell_0.5.0     pillar_1.8.1      rlang_1.0.6      \n[21] utf8_1.2.2        stringi_1.7.8     xfun_0.34         cli_3.4.1        \n[25] withr_2.5.0       magrittr_2.0.3    digest_0.6.30     grid_4.3.1       \n[29] lifecycle_1.0.3   vctrs_0.5.0       evaluate_0.17     glue_1.6.2       \n[33] farver_2.1.1      fansi_1.0.3       colorspace_2.0-3  rmarkdown_2.17   \n[37] purrr_0.3.5       tools_4.3.1       pkgconfig_2.0.3   ellipsis_0.3.2   \n[41] htmltools_0.5.3"
  }
]